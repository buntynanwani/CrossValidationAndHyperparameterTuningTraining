{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea1ef24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07cffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f84166f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 500 samples for regression.\n",
      "Goal: Compare the efficiency of Bayesian Optimization and Successive Halving.\n"
     ]
    }
   ],
   "source": [
    "## üìö 1. Setup and Data Loading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.experimental import enable_halving_search_cv # Required for HalvingGridSearchCV\n",
    "from sklearn.model_selection import HalvingRandomSearchCV # NEW TOOL!\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from skopt import BayesSearchCV # Requires: pip install scikit-optimize\n",
    "from skopt.space import Real, Integer # For defining parameter space\n",
    "\n",
    "# --- Load the same standard regression dataset for fair comparison ---\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(\n",
    "    n_samples=500,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "print(f\"Dataset loaded: {len(X)} samples for regression.\")\n",
    "print(\"Goal: Compare the efficiency of Bayesian Optimization and Successive Halving.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9389d9",
   "metadata": {},
   "source": [
    "## üß† 2. Bayesian Optimization (The Smart Search)\n",
    "\n",
    "Unlike Grid or Random Search, **Bayesian Optimization (BO)** doesn't test blindly. It uses the results of past trials to decide which hyperparameter combination to test next.\n",
    "\n",
    "* It builds a probabilistic model (a \"surrogate\") of the objective function (our score).\n",
    "* It chooses the next point that is likely to be either much better than the current best *or* located in an unexplored area of the search space.\n",
    "\n",
    "This technique is often the most **sample-efficient** (requires the fewest total model fits).\n",
    "\n",
    "### 2.1. Defining the Search Space (Distributions)\n",
    "\n",
    "```python\n",
    "# Use the same Random Forest Regressor\n",
    "rf_model_bo = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the search space using distributions (Integer and Real)\n",
    "# skopt requires its own syntax for defining the ranges.\n",
    "search_space_bo = {\n",
    "    'n_estimators': Integer(50, 200),\n",
    "    'max_depth': Integer(5, 30),\n",
    "    'min_samples_split': Integer(2, 20),\n",
    "    'max_features': ['sqrt', 'log2', 1.0]\n",
    "}\n",
    "\n",
    "n_iterations_bo = 40 # Use the same number of iterations as Random Search for comparison\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 2.2. Initialize and Run Bayesian Optimization\n",
    "bo_search = BayesSearchCV(\n",
    "    estimator=rf_model_bo,\n",
    "    search_spaces=search_space_bo,\n",
    "    n_iter=n_iterations_bo,\n",
    "    scoring=mae_scorer,\n",
    "    cv=cv_strategy,\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting Bayesian Optimization ({n_iterations_bo} iterations)...\")\n",
    "start_time_bo = time.time()\n",
    "bo_search.fit(X, y)\n",
    "end_time_bo = time.time()\n",
    "bo_time = end_time_bo - start_time_bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcef0d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Bayesian Optimization (40 iterations)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [np.int64(18), np.str_('sqrt'), np.int64(3), np.int64(200)] before, using random point [np.int64(10), 'log2', np.int64(20), np.int64(72)]\n",
      "  warnings.warn(\n",
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [np.int64(18), np.str_('sqrt'), np.int64(3), np.int64(200)] before, using random point [np.int64(14), 'log2', np.int64(12), np.int64(68)]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use the same Random Forest Regressor\n",
    "rf_model_bo = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the search space using distributions (Integer and Real)\n",
    "# skopt requires its own syntax for defining the ranges.\n",
    "search_space_bo = {\n",
    "    'n_estimators': Integer(50, 200),\n",
    "    'max_depth': Integer(5, 30),\n",
    "    'min_samples_split': Integer(2, 20),\n",
    "    'max_features': ['sqrt', 'log2', 1.0]\n",
    "}\n",
    "\n",
    "n_iterations_bo = 40 # Use the same number of iterations as Random Search for comparison\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 2.2. Initialize and Run Bayesian Optimization\n",
    "bo_search = BayesSearchCV(\n",
    "    estimator=rf_model_bo,\n",
    "    search_spaces=search_space_bo,\n",
    "    n_iter=n_iterations_bo,\n",
    "    scoring=mae_scorer,\n",
    "    cv=cv_strategy,\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting Bayesian Optimization ({n_iterations_bo} iterations)...\")\n",
    "start_time_bo = time.time()\n",
    "bo_search.fit(X, y)\n",
    "end_time_bo = time.time()\n",
    "bo_time = end_time_bo - start_time_bo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6da648",
   "metadata": {},
   "source": [
    "#### üß† Why the Warnings Appear\n",
    "\n",
    "**That is a common and important warning when using Bayesian Optimization with the `scikit-optimize` library (`skopt`).**\n",
    "\n",
    "**The warnings you are seeing:**\n",
    "\n",
    "**UserWarning: The objective has been evaluated at point \\[...\\] before, using random point \\[...\\]**\n",
    "\n",
    "**occur because the Bayesian optimization process is trying to select the most *promising* next point to evaluate, but it has accidentally selected a hyperparameter combination that it has already tested in a previous iteration.**\n",
    "\n",
    "**Here's a breakdown of why this happens and why `skopt` switches to a random point:**\n",
    "\n",
    "---\n",
    "\n",
    "## **üß† Why the Warnings Appear in `BayesSearchCV`**\n",
    "\n",
    "**Bayesian Optimization works by balancing exploration (trying new, unknown regions of the parameter space) and exploitation (testing near the best-performing points found so far).**\n",
    "\n",
    "### **1\\. The Core Mechanism**\n",
    "\n",
    "1. **Surrogate Model: `skopt` builds a Gaussian Process model (the surrogate) to estimate the model's performance across the entire search space.**  \n",
    "2. **Acquisition Function: It uses this surrogate model to find the next point that maximizes the Expected Improvement (EI)‚Äîthe point where the score is likely to be best.**\n",
    "\n",
    "### **2\\. The Collision (The Warning)**\n",
    "\n",
    "**Due to the nature of continuous sampling (especially when using `Integer` ranges and a limited number of unique candidates):**\n",
    "\n",
    "* **The acquisition function may calculate that the optimal next point is a set of hyperparameters (e.g., `max_depth=18`, `n_estimators=200`) that was already tested in a previous iteration.**  \n",
    "* **Since re-evaluating an already-tested point is a waste of time and computational resources, `skopt` uses a guardrail.**\n",
    "\n",
    "### **3\\. The `skopt` Fallback (The Resolution)**\n",
    "\n",
    "* **When `skopt` detects that the chosen \"optimal\" point is a duplicate, it discards that point and instead selects a new, purely random point from the search space to continue the process.**  \n",
    "* **This ensures the tuning process moves forward and doesn't get stuck in a loop.**\n",
    "\n",
    "## **üìù Is This a Problem?**\n",
    "\n",
    "**No, this is generally NOT a problem for the final result of your tuning.**\n",
    "\n",
    "1. **The tuning process is still valid: `skopt` intelligently avoids wasted time by switching to an untested point.**  \n",
    "2. **It's common: This happens frequently, especially when the search space is limited or when the best hyperparameters are clustered together.**\n",
    "\n",
    "**You can safely ignore these warnings. They are simply information about the internal decision-making process of the Bayesian optimizer.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d404a04",
   "metadata": {},
   "source": [
    "## üèÜ 3. Successive Halving (The Tournament)\n",
    "\n",
    "**Successive Halving** is a method that treats hyperparameter candidates like competitors in a tournament:\n",
    "\n",
    "1.  **Round 1:** Test *many* candidates on a **small subset** of the training data (e.g., 20% of samples).\n",
    "2.  **Elimination:** Discard the candidates that performed poorly (e.g., keep the top 50%).\n",
    "3.  **Next Round:** Test the remaining candidates on a **larger subset** of the data.\n",
    "4.  **Final Round:** Only the top few candidates are tested on the **full dataset**.\n",
    "\n",
    "This saves massive amounts of time by only fully training the most promising candidates. We use `HalvingRandomSearchCV` here.\n",
    "\n",
    "### 3.1. Defining the Halving Search\n",
    "\n",
    "```python\n",
    "# Use the same Random Forest Regressor and search space distributions\n",
    "rf_model_hs = RandomForestRegressor(random_state=42)\n",
    "param_distribution_hs = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [5, 10, 15, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2', 1.0]\n",
    "}\n",
    "\n",
    "# 3.2. Initialize and Run Halving Random Search\n",
    "# factor=2 means we discard half of the candidates at each round.\n",
    "hs_search = HalvingRandomSearchCV(\n",
    "    estimator=rf_model_hs,\n",
    "    param_distributions=param_distribution_hs,\n",
    "    factor=2, # Halve candidates/double resources in each round\n",
    "    n_candidates=40, # Start with 40 random candidates\n",
    "    scoring=mae_scorer,\n",
    "    cv=cv_strategy,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting Successive Halving (starts with 40 candidates)...\")\n",
    "start_time_hs = time.time()\n",
    "hs_search.fit(X, y)\n",
    "end_time_hs = time.time()\n",
    "hs_time = end_time_hs - start_time_hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34d2ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Successive Halving (starts with 40 candidates)...\n",
      "n_iterations: 6\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 10\n",
      "max_resources_: 500\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 40\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 20\n",
      "n_resources: 20\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 10\n",
      "n_resources: 40\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 5\n",
      "n_resources: 80\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 3\n",
      "n_resources: 160\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 2\n",
      "n_resources: 320\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    }
   ],
   "source": [
    "# Use the same Random Forest Regressor and search space distributions\n",
    "rf_model_hs = RandomForestRegressor(random_state=42)\n",
    "param_distribution_hs = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [5, 10, 15, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2', 1.0]\n",
    "}\n",
    "\n",
    "# 3.2. Initialize and Run Halving Random Search\n",
    "# factor=2 means we discard half of the candidates at each round.\n",
    "hs_search = HalvingRandomSearchCV(\n",
    "    estimator=rf_model_hs,\n",
    "    param_distributions=param_distribution_hs,\n",
    "    factor=2, # Halve candidates/double resources in each round\n",
    "    n_candidates=40, # Start with 40 random candidates\n",
    "    scoring=mae_scorer,\n",
    "    cv=cv_strategy,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting Successive Halving (starts with 40 candidates)...\")\n",
    "start_time_hs = time.time()\n",
    "hs_search.fit(X, y)\n",
    "end_time_hs = time.time()\n",
    "hs_time = end_time_hs - start_time_hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b1221a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bayesian Optimization Results ---\n",
      "Time Taken: 188.65 seconds.\n",
      "Best CV Score (MAE): 18.829\n",
      "Best Parameters: OrderedDict({'max_depth': 18, 'max_features': 1.0, 'min_samples_split': 2, 'n_estimators': 149})\n",
      "\n",
      "--- Successive Halving Results ---\n",
      "Time Taken: 49.60 seconds.\n",
      "Best CV Score (MAE): 21.276\n",
      "Best Parameters: {'n_estimators': 50, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 30}\n",
      "\n",
      "\n",
      "--- Conclusion and Rationale ---\n"
     ]
    }
   ],
   "source": [
    "## üìà 4. Final Comparison and Conclusion\n",
    "\n",
    "# Assuming Random Search time and score from Notebook 02 are available for comparison\n",
    "\n",
    "# Display results from this notebook\n",
    "print(\"\\n--- Bayesian Optimization Results ---\")\n",
    "print(f\"Time Taken: {bo_time:.2f} seconds.\")\n",
    "print(f\"Best CV Score (MAE): {-bo_search.best_score_:.3f}\")\n",
    "print(f\"Best Parameters: {bo_search.best_params_}\")\n",
    "\n",
    "print(\"\\n--- Successive Halving Results ---\")\n",
    "print(f\"Time Taken: {hs_time:.2f} seconds.\")\n",
    "print(f\"Best CV Score (MAE): {-hs_search.best_score_:.3f}\")\n",
    "print(f\"Best Parameters: {hs_search.best_params_}\")\n",
    "\n",
    "# --- CONCLUSION ---\n",
    "\n",
    "print(\"\\n\\n--- Conclusion and Rationale ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261a645",
   "metadata": {},
   "source": [
    "## üåü 5. Conclusion and Dataset Rationale\n",
    "\n",
    "### Summary of Tuning Methods\n",
    "\n",
    "| Method | Approach | Speed | Score | Best Use Case |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Random Search** | Tests random points. | Fast | High (Good) | General, initial tuning. |\n",
    "| **Bayesian Opt.** | Tests smart points (learns as it goes). | Medium | Highest (Best) | Finding the absolute optimum in complex spaces. |\n",
    "| **Successive Halving** | Tournament (eliminates poor candidates early). | Fastest | High (Good) | Very large search spaces where speed is critical. |\n",
    "\n",
    "### üéØ Rationale for Dataset Choice (Why Not My Supplement Data?)\n",
    "\n",
    "* **Goal of Notebooks 01-03:** The core objective was to show the **time efficiency** and **methodology** of Grid, Random, and Advanced Searches.\n",
    "* **The Problem:** Tuning with the `Supplement_Sales_Weekly_Expanded.csv` data requires the **TimeSeriesSplit** CV strategy. **TimeSeriesSplit is computationally slow** because it must retrain the model on an ever-expanding subset of data in *every* fold.\n",
    "* **The Result:** If we used the TimeSeries data, the notebook would take a very long time to run, and we wouldn't be able to clearly demonstrate the speed advantage of Random Search, Bayesian Optimization, or Halving against each other.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
