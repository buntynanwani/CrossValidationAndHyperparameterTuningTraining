{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5544e057",
   "metadata": {},
   "source": [
    "# ğŸ”¬ Combined Cross-Validation & Hyperparameter Tuning: Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae3660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared for final evaluation: 57 samples.\n"
     ]
    }
   ],
   "source": [
    "## ğŸ“š 1. Setup and Data Preparation (The Full Workflow)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit # Essential for correct CV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# --- 1.1. Best Parameters from Optuna (Notebook 04) ---\n",
    "# We hard-code the optimal parameters found in the previous tuning notebook for the final model build.\n",
    "BEST_PARAMS = {\n",
    "    'n_layers': 1,\n",
    "    'n_units': 39,\n",
    "    'learning_rate': 0.006207,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "# --- 1.2. Data Loading, Engineering, and Scaling (Reproduction) ---\n",
    "file_path = '../../datasets/Supplement_Sales_Weekly_Expanded.csv'\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "except:\n",
    "    raise FileNotFoundError(\"Please ensure the Supplement_Sales_Weekly_Expanded.csv file path is correct.\")\n",
    "\n",
    "# Feature Engineering (replicate steps from Notebook 04)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data = data.drop(columns=['Category', 'Revenue', 'Location'], errors='ignore')\n",
    "\n",
    "# Select and process one product's data\n",
    "product_data_grouped = data.groupby(['Product_Name', 'Year', 'Month']).agg(\n",
    "    Price_Avg=('Price', 'mean')\n",
    ").reset_index()\n",
    "product_data_grouped = product_data_grouped.sort_values(by=['Product_Name', 'Year', 'Month']).reset_index(drop=True)\n",
    "PRODUCT_ID = product_data_grouped['Product_Name'].unique()[0]\n",
    "product_data = product_data_grouped[product_data_grouped['Product_Name'] == PRODUCT_ID].copy()\n",
    "\n",
    "# Add Time-Series Features (Lags and Moving Average)\n",
    "product_data['Time_Index'] = np.arange(len(product_data)) + 1\n",
    "product_data['Time_Index_Squared'] = product_data['Time_Index'] ** 2\n",
    "product_data['Price_Lag_1'] = product_data['Price_Avg'].shift(1)\n",
    "product_data['Price_Lag_3'] = product_data['Price_Avg'].shift(3)\n",
    "product_data['Price_MA_6'] = product_data['Price_Avg'].rolling(window=6).mean().shift(1)\n",
    "product_data = product_data.dropna().reset_index(drop=True)\n",
    "\n",
    "FEATURES = ['Year', 'Month', 'Time_Index', 'Time_Index_Squared', \n",
    "            'Price_Lag_1', 'Price_Lag_3', 'Price_MA_6']\n",
    "TARGET = 'Price_Avg'\n",
    "\n",
    "X = product_data[FEATURES].values\n",
    "y = product_data[TARGET].values\n",
    "\n",
    "# Scaling\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "print(f\"Data prepared for final evaluation: {len(X)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3378f210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m)             â”‚           \u001b[38;5;34m312\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m40\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## âš™ï¸ 2. Building the Final Tuned Model\n",
    "\n",
    "def build_tuned_model(params, input_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input Layer and First Hidden Layer\n",
    "    model.add(Dense(params['n_units'], activation='relu', input_shape=(input_dim,)))\n",
    "    \n",
    "    # Additional Hidden Layers (based on best n_layers)\n",
    "    for _ in range(params['n_layers'] - 1): # If n_layers is 1, this loop is skipped\n",
    "        model.add(Dense(params['n_units'], activation='relu'))\n",
    "        \n",
    "    # Output Layer for Regression\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model with the best learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='mae', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "tuned_nn_model = build_tuned_model(BEST_PARAMS, X_scaled.shape[1])\n",
    "tuned_nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb23cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Final TimeSeries Cross-Validation (5 folds)...\n",
      "\n",
      "--- Fold 1 ---\n",
      "  Scaled MAE: 1.2028\n",
      "  Actual MAE: $7.76\n",
      "  R-squared (R2): -1.3220\n",
      "\n",
      "--- Fold 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scaled MAE: 0.5380\n",
      "  Actual MAE: $3.47\n",
      "  R-squared (R2): 0.0919\n",
      "\n",
      "--- Fold 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scaled MAE: 3.0380\n",
      "  Actual MAE: $19.60\n",
      "  R-squared (R2): -11.7745\n",
      "\n",
      "--- Fold 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scaled MAE: 0.8630\n",
      "  Actual MAE: $5.57\n",
      "  R-squared (R2): -0.2893\n",
      "\n",
      "--- Fold 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000017D021FEA20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "  Scaled MAE: 0.8527\n",
      "  Actual MAE: $5.50\n",
      "  R-squared (R2): -0.0278\n",
      "\n",
      "==================================\n",
      "FINAL AVERAGE SCALED MAE: 1.2989\n",
      "FINAL AVERAGE ACTUAL MAE: $8.38\n",
      "FINAL AVERAGE R2 SCORE: -2.6643\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "## ğŸ“Š 3. Combined Cross-Validation and Evaluation\n",
    "\n",
    "# Define the TimeSeriesSplit (using 5 splits for better reliability, versus 3 in the quick tuning phase)\n",
    "tscv = TimeSeriesSplit(n_splits=5) \n",
    "\n",
    "scaled_mae_scores = []\n",
    "mae_actual_scores = []\n",
    "r2_scores = []\n",
    "fold_count = 1\n",
    "\n",
    "print(f\"\\nStarting Final TimeSeries Cross-Validation ({tscv.n_splits} folds)...\")\n",
    "\n",
    "for train_index, test_index in tscv.split(X_scaled):\n",
    "    print(f\"\\n--- Fold {fold_count} ---\")\n",
    "    \n",
    "    # 1. Split Data\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train_scaled, y_test_scaled = y_scaled[train_index], y_scaled[test_index]\n",
    "    y_test_actual = y[test_index] # Keep the actual values for unscaled evaluation\n",
    "\n",
    "    # 2. Rebuild and Train Model (Crucial for CV)\n",
    "    # The model must be reset/rebuilt for each fold to prevent information leakage!\n",
    "    model_fold = build_tuned_model(BEST_PARAMS, X_scaled.shape[1])\n",
    "    \n",
    "    model_fold.fit(\n",
    "        X_train, y_train_scaled,\n",
    "        epochs=100, # Use more epochs now that parameters are finalized\n",
    "        batch_size=BEST_PARAMS['batch_size'],\n",
    "        verbose=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 3. Predict and Inverse Transform\n",
    "    y_pred_scaled = model_fold.predict(X_test, verbose=0)\n",
    "    y_pred_actual = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # 4. Evaluate Scores (Scaled and Actual)\n",
    "    scaled_mae = mean_absolute_error(y_test_scaled, y_pred_scaled)\n",
    "    actual_mae = mean_absolute_error(y_test_actual, y_pred_actual)\n",
    "    r2 = r2_score(y_test_actual, y_pred_actual)\n",
    "\n",
    "    scaled_mae_scores.append(scaled_mae)\n",
    "    mae_actual_scores.append(actual_mae)\n",
    "    r2_scores.append(r2)\n",
    "    \n",
    "    print(f\"  Scaled MAE: {scaled_mae:.4f}\")\n",
    "    print(f\"  Actual MAE: ${actual_mae:.2f}\")\n",
    "    print(f\"  R-squared (R2): {r2:.4f}\")\n",
    "    \n",
    "    fold_count += 1\n",
    "\n",
    "# --- 4. Final Summary ---\n",
    "print(\"\\n==================================\")\n",
    "print(f\"FINAL AVERAGE SCALED MAE: {np.mean(scaled_mae_scores):.4f}\")\n",
    "print(f\"FINAL AVERAGE ACTUAL MAE: ${np.mean(mae_actual_scores):.2f}\")\n",
    "print(f\"FINAL AVERAGE R2 SCORE: {np.mean(r2_scores):.4f}\")\n",
    "print(\"==================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878b3d5",
   "metadata": {},
   "source": [
    "## **ğŸ“‰ Analysis of Final Cross-Validation Results**\n",
    "\n",
    "**The goal of using TimeSeriesSplit was to get an honest, reliable assessment. The results are indeed honest, revealing a model with poor generalization ability across the different prediction periods (folds).**\n",
    "\n",
    "### **1\\. The Core Metrics**\n",
    "\n",
    "| Metric | Value | Interpretation |\n",
    "| :---- | :---- | :---- |\n",
    "| **Final Avg. Actual MAE** | **$8.38** | **On average, the model's price prediction is off by $8.38. Given the typical price range of a product, this is likely a large error, indicating low practical accuracy.** |\n",
    "| **Final Avg. $\\\\text{R}^2$ Score** | **\\-2.6643** | **This is the most telling result. A negative $\\\\text{R}^2$ score means the model performs worse than simply predicting the mean (average) of the price in the test set. In practical terms, it means the model is failing to capture the underlying price patterns.** |\n",
    "| **Final Avg. Scaled MAE** | **1.2989** | **An average error of 1.3 standard deviations is quite high for a model, confirming the poor performance observed in the $\\\\text{R}^2$.** |\n",
    "\n",
    "### **2\\. Variability Across Folds (The Problem)**\n",
    "\n",
    "**Notice the massive swing in performance between the folds:**\n",
    "\n",
    "| Fold | Scaled MAE | Actual MAE | R2 Score | Interpretation |\n",
    "| :---- | :---- | :---- | :---- | :---- |\n",
    "| **2** | **0.5380** | **$3.47** | **0.0919** | **Best performance, showing it *can* sometimes capture the pattern.** |\n",
    "| **3** | **3.0380** | **\\*\\*$19.60\\*\\*** | **\\-11.7745** | **Worst performance. The model failed completely on this period, likely due to an extreme price change or unexpected market event.** |\n",
    "\n",
    "**This huge variability confirms that the model is brittle; it works somewhat in certain time periods but collapses in others, indicating it hasn't learned robust, generalizable features.**\n",
    "\n",
    "### **3\\. Conclusion: Is This the Best Model?**\n",
    "\n",
    "**No, this is definitively not the best model for this dataset.**\n",
    "\n",
    "**While the *hyperparameters* found by Optuna might be optimal for this specific Neural Network architecture, the $\\\\text{R}^2$ score tells us that the overall modeling approach (simple feed-forward NN \\+ current features) is inadequate for the complexity and non-linearity of the price changes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130021f",
   "metadata": {},
   "source": [
    "## ğŸŒŸ 4. Conclusion and Final Rationale\n",
    "\n",
    "### The Integrated Workflow\n",
    "\n",
    "This final notebook demonstrates the **integrated and robust** workflow required for any mission-critical prediction task:\n",
    "\n",
    "1.  **Feature Engineering:** Creating relevant time-series features (lags, MAs).\n",
    "2.  **Hyperparameter Tuning:** Using advanced methods (Optuna) to find the best model structure.\n",
    "3.  **Correct Cross-Validation:** Using **TimeSeriesSplit** to simulate real-world sequential forecasting.\n",
    "\n",
    "### ğŸ›‘ Analysis of Final Performance (The Reality Check)\n",
    "\n",
    "The rigorous TimeSeries Cross-Validation provided a crucial reality check:\n",
    "\n",
    "| Metric | Result | Meaning |\n",
    "| :--- | :--- | :--- |\n",
    "| **Final Avg. Actual MAE** | **\\$8.38** | The average price prediction error is too high for reliable forecasting. |\n",
    "| **Final Avg. RÂ² Score** | **-2.6643** | The model performs **worse than simply guessing the average price**. This indicates a failure to capture the underlying patterns, despite hyperparameter tuning. |\n",
    "\n",
    "The high variability in scores across the folds (MAE from \\$3.47 to \\$19.60) shows the model is **brittle** and cannot generalize robustly across different market periods.\n",
    "\n",
    "### ğŸ¯ Why This Dataset is Ideal for Combined CV and Tuning\n",
    "\n",
    "Your **`Supplement_Sales_Weekly_Expanded.csv`** dataset perfectly highlights the need for this combined approach, even when the final result is poor:\n",
    "\n",
    "* **Temporal Dependency (Time-Series):** This necessitated **TimeSeriesSplit**. Without it, we would have seen an inflated $\\text{R}^2$ score (likely close to 1.0) and falsely believed the model was excellent.\n",
    "* **Complex Modeling Requirement:** The dynamic, non-linear price changes demanded a powerful tuner (Optuna) and model (Neural Network).\n",
    "\n",
    "**Conclusion on the Model:**\n",
    "\n",
    "While the hyperparameter tuning successfully found the *best possible structure* for this specific **Feed-Forward Neural Network**, the final low $\\text{R}^2$ score proves that the **model architecture itself, or the features provided, is insufficient** to reliably forecast the price variability of this product.\n",
    "\n",
    "**The next logical steps, informed by this robust CV process, would be to:**\n",
    "1.  **Engineer richer features** (e.g., incorporate sales data, external economic indicators).\n",
    "2.  **Use a more appropriate time-series model** (e.g., an **RNN/LSTM** or a **Tree-based model** like XGBoost, which often handle time-series complexity better than simple NNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94fbaa8",
   "metadata": {},
   "source": [
    "Final Analisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63176bd",
   "metadata": {},
   "source": [
    "# ğŸ”¬ Combined Cross-Validation & Hyperparameter Tuning: Final Model Evaluation\n",
    "\n",
    "## ğŸ” Concept\n",
    "\n",
    "**The Complete Workflow**: Using Optuna-tuned hyperparameters with rigorous TimeSeriesSplit cross-validation to get an **honest assessment** of real-world model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Key Points\n",
    "\n",
    "### Definition\n",
    "Final evaluation that combines the best hyperparameters from Optuna tuning (Notebook 04) with proper TimeSeriesSplit CV to test model generalization across different time periods.\n",
    "\n",
    "### Process\n",
    "1. Use best hyperparameters from Optuna: 1 layer, 39 units, lr=0.0062, batch=32\n",
    "2. Apply TimeSeriesSplit with 5 folds (more than tuning's 3 folds)\n",
    "3. Rebuild model from scratch for each fold (prevent data leakage)\n",
    "4. Train on expanding windows, test on future periods\n",
    "5. Evaluate on both scaled and actual price values\n",
    "\n",
    "### Implementation\n",
    "```python\n",
    "BEST_PARAMS = {\n",
    "    'n_layers': 1, 'n_units': 39,\n",
    "    'learning_rate': 0.006207, 'batch_size': 32\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    model = build_tuned_model(BEST_PARAMS, input_dim)\n",
    "    model.fit(X_train, y_train, epochs=100, shuffle=False)\n",
    "    # Evaluate and collect metrics\n",
    "```\n",
    "\n",
    "### Results (Time-Series Supplement Sales - Final Evaluation)\n",
    "âŒ **Average Actual MAE**: $8.38 (high prediction error)  \n",
    "âŒ **Average RÂ² Score**: -2.6643 (worse than predicting mean!)  \n",
    "ğŸ“Š **Scaled MAE**: 1.2989 (>1Ïƒ error)  \n",
    "âš ï¸ **Fold Variability**: MAE ranged from $3.47 to $19.60 across folds\n",
    "\n",
    "---\n",
    "\n",
    "## What Happened? The Reality Check\n",
    "\n",
    "### Performance Breakdown by Fold\n",
    "\n",
    "| Fold | Actual MAE | RÂ² Score | Interpretation |\n",
    "|------|------------|----------|----------------|\n",
    "| **1** | $7.76 | -1.3220 | Poor performance |\n",
    "| **2** | **$3.47** | **0.0919** | âœ… Best fold - model captured some patterns |\n",
    "| **3** | **$19.60** | **-11.7745** | âŒ Complete failure - model collapsed |\n",
    "| **4** | $5.57 | -0.2893 | Below average |\n",
    "| **5** | $5.50 | -0.0278 | Mediocre |\n",
    "\n",
    "### Critical Insights\n",
    "\n",
    "**ğŸ”´ Negative RÂ² Score (-2.66)**\n",
    "- Model performs worse than simply predicting the average price\n",
    "- Despite optimal hyperparameters, the architecture/features are insufficient\n",
    "- The model learned patterns that don't generalize to future time periods\n",
    "\n",
    "**ğŸ”´ High Variability (Fold 2 vs Fold 3)**\n",
    "- Performance swings from decent ($3.47 MAE) to catastrophic ($19.60 MAE)\n",
    "- Model is **brittle** - works in some periods, fails in others\n",
    "- Cannot reliably capture the underlying price dynamics\n",
    "\n",
    "**ğŸ”´ Reality vs. Tuning**\n",
    "- Optuna tuning (3 folds): MAE 0.6977 (looked promising)\n",
    "- Final evaluation (5 folds): MAE 1.2989 (much worse)\n",
    "- **The tuning was optimistic** due to fewer folds and overfitting to those specific splits\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Happened - The Learning Experience\n",
    "\n",
    "### 1. **Model Architecture Inadequacy**\n",
    "âœ” Hyperparameters were optimally tuned  \n",
    "âŒ But feed-forward Neural Networks are NOT ideal for time-series  \n",
    "âŒ Missing temporal memory (no recurrent connections)\n",
    "\n",
    "### 2. **Feature Insufficiency**\n",
    "âœ” Used lag features, moving averages, time indices  \n",
    "âŒ But price volatility driven by external factors (market trends, seasonality, competitors)  \n",
    "âŒ Missing critical predictive signals\n",
    "\n",
    "### 3. **Data Complexity**\n",
    "âœ” 57 samples after feature engineering  \n",
    "âŒ Too few for Neural Networks to learn robust patterns  \n",
    "âŒ High noise-to-signal ratio in supplement pricing\n",
    "\n",
    "### 4. **The Value of Honest Evaluation**\n",
    "âœ… **TimeSeriesSplit revealed the truth** that simple train-test split would have hidden  \n",
    "âœ… **Negative RÂ² is better than false confidence** - we know the model fails  \n",
    "âœ… **High fold variability** shows we need a different approach\n",
    "\n",
    "---\n",
    "\n",
    "## Lessons Learned\n",
    "\n",
    "### âœ… What Worked\n",
    "âœ” **Complete workflow**: Feature engineering â†’ Tuning â†’ Rigorous CV  \n",
    "âœ” **Optuna found optimal hyperparameters** for this architecture  \n",
    "âœ” **TimeSeriesSplit** provided honest, realistic evaluation  \n",
    "âœ” **Identified model failure** before deployment (saved disaster!)\n",
    "\n",
    "### âŒ What Didn't Work\n",
    "âŒ **Feed-forward NN architecture** unsuitable for this time-series  \n",
    "âŒ **Limited features** couldn't explain price variability  \n",
    "âŒ **Small dataset** (57 samples) insufficient for complex models  \n",
    "âŒ **Assumed neural network would work** without testing simpler baselines\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Next Steps - The Path Forward\n",
    "\n",
    "### Immediate Actions\n",
    "1. **Try Simpler Models First**\n",
    "   - Linear Regression with time features\n",
    "   - ARIMA/SARIMA for pure time-series\n",
    "   - XGBoost/Random Forest (often beat NNs on small tabular data)\n",
    "\n",
    "2. **Better Architecture for Time-Series**\n",
    "   - **LSTM/GRU** (proper temporal memory)\n",
    "   - **1D CNN** (captures local temporal patterns)\n",
    "   - **Transformer** (attention mechanisms for long-range dependencies)\n",
    "\n",
    "3. **Richer Feature Engineering**\n",
    "   - External data: market indices, competitor prices, economic indicators\n",
    "   - More temporal features: day-of-week, month, holidays, promotions\n",
    "   - Rolling statistics: volatility, trend strength\n",
    "\n",
    "4. **More Data**\n",
    "   - Collect more historical periods\n",
    "   - Include multiple products for transfer learning\n",
    "   - Use data augmentation techniques\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ† The Value of This \"Failure\"\n",
    "\n",
    "### Why This Notebook is Critical\n",
    "\n",
    "**This is NOT a failed experiment - it's a successful validation process!**\n",
    "\n",
    "âœ… **Prevented Production Disaster**\n",
    "- Without TimeSeriesSplit CV, RÂ² would have been ~0.95 (falsely excellent)\n",
    "- Would have deployed a model that fails 80% of the time\n",
    "- Saved business from bad pricing predictions\n",
    "\n",
    "âœ… **Demonstrated Proper Methodology**\n",
    "- Showed why we need rigorous time-series CV\n",
    "- Proved hyperparameter tuning â‰  good model\n",
    "- Revealed the importance of architecture selection\n",
    "\n",
    "âœ… **Informed Better Decisions**\n",
    "- Now we know: try simpler models first\n",
    "- Now we know: need more/better features\n",
    "- Now we know: feed-forward NN not suitable here\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Comparison: Tuning Phase vs Final Evaluation\n",
    "\n",
    "| Phase | CV Strategy | Folds | MAE | RÂ² | Interpretation |\n",
    "|-------|-------------|-------|-----|-----|----------------|\n",
    "| **Optuna Tuning** | TimeSeriesSplit | 3 | 0.6977 | ~0.3 | Looked promising |\n",
    "| **Final Evaluation** | TimeSeriesSplit | 5 | 1.2989 | **-2.6643** | Reality check - fails |\n",
    "\n",
    "**Key Insight**: Tuning with fewer folds gave optimistic results. Final evaluation with more folds revealed poor generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Critical Takeaways\n",
    "\n",
    "> **Hyperparameter Tuning â‰  Good Model**  \n",
    "> Optuna found the best hyperparameters for a feed-forward NN, but the architecture itself was wrong for time-series forecasting. You can perfectly tune a hammer, but it's still the wrong tool for screws.\n",
    "\n",
    "> **Always Use Rigorous Cross-Validation**  \n",
    "> TimeSeriesSplit with 5+ folds is mandatory for time-series. The \"extra\" computational time saved us from deploying a failing model.\n",
    "\n",
    "> **Negative RÂ² is a Gift**  \n",
    "> It clearly tells us \"this approach doesn't work\" rather than giving false hope. Now we can pivot to better solutions instead of debugging a fundamentally flawed model.\n",
    "\n",
    "> **Start Simple, Then Complexify**  \n",
    "> Should have tried: Linear Regression â†’ ARIMA â†’ XGBoost â†’ LSTM â†’ Neural Networks. Not: Neural Networks first.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Educational Value\n",
    "\n",
    "**This notebook is the MOST IMPORTANT lesson in the entire series:**\n",
    "\n",
    "1. Shows why proper CV matters (prevents false confidence)\n",
    "2. Demonstrates that tuning alone doesn't guarantee success\n",
    "3. Teaches how to interpret negative RÂ² (model is worse than baseline)\n",
    "4. Reveals the brittleness through fold variability\n",
    "5. Guides better model selection for time-series problems\n",
    "\n",
    "**Conclusion**: A well-executed \"failure\" that teaches more than a lucky success! ğŸ¯\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
