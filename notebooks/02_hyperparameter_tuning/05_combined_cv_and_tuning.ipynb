{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5544e057",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae3660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared for final evaluation: 57 samples.\n"
     ]
    }
   ],
   "source": [
    "## ğŸ“š 1. Setup and Data Preparation (The Full Workflow)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit # Essential for correct CV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# --- 1.1. Best Parameters from Optuna (Notebook 04) ---\n",
    "# We hard-code the optimal parameters found in the previous tuning notebook for the final model build.\n",
    "BEST_PARAMS = {\n",
    "    'n_layers': 1,\n",
    "    'n_units': 39,\n",
    "    'learning_rate': 0.006207,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "# --- 1.2. Data Loading, Engineering, and Scaling (Reproduction) ---\n",
    "file_path = '../../datasets/Supplement_Sales_Weekly_Expanded.csv'\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "except:\n",
    "    raise FileNotFoundError(\"Please ensure the Supplement_Sales_Weekly_Expanded.csv file path is correct.\")\n",
    "\n",
    "# Feature Engineering (replicate steps from Notebook 04)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data = data.drop(columns=['Category', 'Revenue', 'Location'], errors='ignore')\n",
    "\n",
    "# Select and process one product's data\n",
    "product_data_grouped = data.groupby(['Product_Name', 'Year', 'Month']).agg(\n",
    "    Price_Avg=('Price', 'mean')\n",
    ").reset_index()\n",
    "product_data_grouped = product_data_grouped.sort_values(by=['Product_Name', 'Year', 'Month']).reset_index(drop=True)\n",
    "PRODUCT_ID = product_data_grouped['Product_Name'].unique()[0]\n",
    "product_data = product_data_grouped[product_data_grouped['Product_Name'] == PRODUCT_ID].copy()\n",
    "\n",
    "# Add Time-Series Features (Lags and Moving Average)\n",
    "product_data['Time_Index'] = np.arange(len(product_data)) + 1\n",
    "product_data['Time_Index_Squared'] = product_data['Time_Index'] ** 2\n",
    "product_data['Price_Lag_1'] = product_data['Price_Avg'].shift(1)\n",
    "product_data['Price_Lag_3'] = product_data['Price_Avg'].shift(3)\n",
    "product_data['Price_MA_6'] = product_data['Price_Avg'].rolling(window=6).mean().shift(1)\n",
    "product_data = product_data.dropna().reset_index(drop=True)\n",
    "\n",
    "FEATURES = ['Year', 'Month', 'Time_Index', 'Time_Index_Squared', \n",
    "            'Price_Lag_1', 'Price_Lag_3', 'Price_MA_6']\n",
    "TARGET = 'Price_Avg'\n",
    "\n",
    "X = product_data[FEATURES].values\n",
    "y = product_data[TARGET].values\n",
    "\n",
    "# Scaling\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "print(f\"Data prepared for final evaluation: {len(X)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3378f210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m)             â”‚           \u001b[38;5;34m312\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m40\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## âš™ï¸ 2. Building the Final Tuned Model\n",
    "\n",
    "def build_tuned_model(params, input_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input Layer and First Hidden Layer\n",
    "    model.add(Dense(params['n_units'], activation='relu', input_shape=(input_dim,)))\n",
    "    \n",
    "    # Additional Hidden Layers (based on best n_layers)\n",
    "    for _ in range(params['n_layers'] - 1): # If n_layers is 1, this loop is skipped\n",
    "        model.add(Dense(params['n_units'], activation='relu'))\n",
    "        \n",
    "    # Output Layer for Regression\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model with the best learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='mae', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "tuned_nn_model = build_tuned_model(BEST_PARAMS, X_scaled.shape[1])\n",
    "tuned_nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb23cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Final TimeSeries Cross-Validation (5 folds)...\n",
      "\n",
      "--- Fold 1 ---\n",
      "  Scaled MAE: 1.2028\n",
      "  Actual MAE: $7.76\n",
      "  R-squared (R2): -1.3220\n",
      "\n",
      "--- Fold 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scaled MAE: 0.5380\n",
      "  Actual MAE: $3.47\n",
      "  R-squared (R2): 0.0919\n",
      "\n",
      "--- Fold 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scaled MAE: 3.0380\n",
      "  Actual MAE: $19.60\n",
      "  R-squared (R2): -11.7745\n",
      "\n",
      "--- Fold 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scaled MAE: 0.8630\n",
      "  Actual MAE: $5.57\n",
      "  R-squared (R2): -0.2893\n",
      "\n",
      "--- Fold 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000017D021FEA20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "  Scaled MAE: 0.8527\n",
      "  Actual MAE: $5.50\n",
      "  R-squared (R2): -0.0278\n",
      "\n",
      "==================================\n",
      "FINAL AVERAGE SCALED MAE: 1.2989\n",
      "FINAL AVERAGE ACTUAL MAE: $8.38\n",
      "FINAL AVERAGE R2 SCORE: -2.6643\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "## ğŸ“Š 3. Combined Cross-Validation and Evaluation\n",
    "\n",
    "# Define the TimeSeriesSplit (using 5 splits for better reliability, versus 3 in the quick tuning phase)\n",
    "tscv = TimeSeriesSplit(n_splits=5) \n",
    "\n",
    "scaled_mae_scores = []\n",
    "mae_actual_scores = []\n",
    "r2_scores = []\n",
    "fold_count = 1\n",
    "\n",
    "print(f\"\\nStarting Final TimeSeries Cross-Validation ({tscv.n_splits} folds)...\")\n",
    "\n",
    "for train_index, test_index in tscv.split(X_scaled):\n",
    "    print(f\"\\n--- Fold {fold_count} ---\")\n",
    "    \n",
    "    # 1. Split Data\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train_scaled, y_test_scaled = y_scaled[train_index], y_scaled[test_index]\n",
    "    y_test_actual = y[test_index] # Keep the actual values for unscaled evaluation\n",
    "\n",
    "    # 2. Rebuild and Train Model (Crucial for CV)\n",
    "    # The model must be reset/rebuilt for each fold to prevent information leakage!\n",
    "    model_fold = build_tuned_model(BEST_PARAMS, X_scaled.shape[1])\n",
    "    \n",
    "    model_fold.fit(\n",
    "        X_train, y_train_scaled,\n",
    "        epochs=100, # Use more epochs now that parameters are finalized\n",
    "        batch_size=BEST_PARAMS['batch_size'],\n",
    "        verbose=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 3. Predict and Inverse Transform\n",
    "    y_pred_scaled = model_fold.predict(X_test, verbose=0)\n",
    "    y_pred_actual = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # 4. Evaluate Scores (Scaled and Actual)\n",
    "    scaled_mae = mean_absolute_error(y_test_scaled, y_pred_scaled)\n",
    "    actual_mae = mean_absolute_error(y_test_actual, y_pred_actual)\n",
    "    r2 = r2_score(y_test_actual, y_pred_actual)\n",
    "\n",
    "    scaled_mae_scores.append(scaled_mae)\n",
    "    mae_actual_scores.append(actual_mae)\n",
    "    r2_scores.append(r2)\n",
    "    \n",
    "    print(f\"  Scaled MAE: {scaled_mae:.4f}\")\n",
    "    print(f\"  Actual MAE: ${actual_mae:.2f}\")\n",
    "    print(f\"  R-squared (R2): {r2:.4f}\")\n",
    "    \n",
    "    fold_count += 1\n",
    "\n",
    "# --- 4. Final Summary ---\n",
    "print(\"\\n==================================\")\n",
    "print(f\"FINAL AVERAGE SCALED MAE: {np.mean(scaled_mae_scores):.4f}\")\n",
    "print(f\"FINAL AVERAGE ACTUAL MAE: ${np.mean(mae_actual_scores):.2f}\")\n",
    "print(f\"FINAL AVERAGE R2 SCORE: {np.mean(r2_scores):.4f}\")\n",
    "print(\"==================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878b3d5",
   "metadata": {},
   "source": [
    "## **ğŸ“‰ Analysis of Final Cross-Validation Results**\n",
    "\n",
    "**The goal of using TimeSeriesSplit was to get an honest, reliable assessment. The results are indeed honest, revealing a model with poor generalization ability across the different prediction periods (folds).**\n",
    "\n",
    "### **1\\. The Core Metrics**\n",
    "\n",
    "| Metric | Value | Interpretation |\n",
    "| :---- | :---- | :---- |\n",
    "| **Final Avg. Actual MAE** | **$8.38** | **On average, the model's price prediction is off by $8.38. Given the typical price range of a product, this is likely a large error, indicating low practical accuracy.** |\n",
    "| **Final Avg. $\\\\text{R}^2$ Score** | **\\-2.6643** | **This is the most telling result. A negative $\\\\text{R}^2$ score means the model performs worse than simply predicting the mean (average) of the price in the test set. In practical terms, it means the model is failing to capture the underlying price patterns.** |\n",
    "| **Final Avg. Scaled MAE** | **1.2989** | **An average error of 1.3 standard deviations is quite high for a model, confirming the poor performance observed in the $\\\\text{R}^2$.** |\n",
    "\n",
    "### **2\\. Variability Across Folds (The Problem)**\n",
    "\n",
    "**Notice the massive swing in performance between the folds:**\n",
    "\n",
    "| Fold | Scaled MAE | Actual MAE | R2 Score | Interpretation |\n",
    "| :---- | :---- | :---- | :---- | :---- |\n",
    "| **2** | **0.5380** | **$3.47** | **0.0919** | **Best performance, showing it *can* sometimes capture the pattern.** |\n",
    "| **3** | **3.0380** | **\\*\\*$19.60\\*\\*** | **\\-11.7745** | **Worst performance. The model failed completely on this period, likely due to an extreme price change or unexpected market event.** |\n",
    "\n",
    "**This huge variability confirms that the model is brittle; it works somewhat in certain time periods but collapses in others, indicating it hasn't learned robust, generalizable features.**\n",
    "\n",
    "### **3\\. Conclusion: Is This the Best Model?**\n",
    "\n",
    "**No, this is definitively not the best model for this dataset.**\n",
    "\n",
    "**While the *hyperparameters* found by Optuna might be optimal for this specific Neural Network architecture, the $\\\\text{R}^2$ score tells us that the overall modeling approach (simple feed-forward NN \\+ current features) is inadequate for the complexity and non-linearity of the price changes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130021f",
   "metadata": {},
   "source": [
    "## ğŸŒŸ 4. Conclusion and Final Rationale\n",
    "\n",
    "### The Integrated Workflow\n",
    "\n",
    "This final notebook demonstrates the **integrated and robust** workflow required for any mission-critical prediction task:\n",
    "\n",
    "1.  **Feature Engineering:** Creating relevant time-series features (lags, MAs).\n",
    "2.  **Hyperparameter Tuning:** Using advanced methods (Optuna) to find the best model structure.\n",
    "3.  **Correct Cross-Validation:** Using **TimeSeriesSplit** to simulate real-world sequential forecasting.\n",
    "\n",
    "### ğŸ›‘ Analysis of Final Performance (The Reality Check)\n",
    "\n",
    "The rigorous TimeSeries Cross-Validation provided a crucial reality check:\n",
    "\n",
    "| Metric | Result | Meaning |\n",
    "| :--- | :--- | :--- |\n",
    "| **Final Avg. Actual MAE** | **\\$8.38** | The average price prediction error is too high for reliable forecasting. |\n",
    "| **Final Avg. RÂ² Score** | **-2.6643** | The model performs **worse than simply guessing the average price**. This indicates a failure to capture the underlying patterns, despite hyperparameter tuning. |\n",
    "\n",
    "The high variability in scores across the folds (MAE from \\$3.47 to \\$19.60) shows the model is **brittle** and cannot generalize robustly across different market periods.\n",
    "\n",
    "### ğŸ¯ Why This Dataset is Ideal for Combined CV and Tuning\n",
    "\n",
    "Your **`Supplement_Sales_Weekly_Expanded.csv`** dataset perfectly highlights the need for this combined approach, even when the final result is poor:\n",
    "\n",
    "* **Temporal Dependency (Time-Series):** This necessitated **TimeSeriesSplit**. Without it, we would have seen an inflated $\\text{R}^2$ score (likely close to 1.0) and falsely believed the model was excellent.\n",
    "* **Complex Modeling Requirement:** The dynamic, non-linear price changes demanded a powerful tuner (Optuna) and model (Neural Network).\n",
    "\n",
    "**Conclusion on the Model:**\n",
    "\n",
    "While the hyperparameter tuning successfully found the *best possible structure* for this specific **Feed-Forward Neural Network**, the final low $\\text{R}^2$ score proves that the **model architecture itself, or the features provided, is insufficient** to reliably forecast the price variability of this product.\n",
    "\n",
    "**The next logical steps, informed by this robust CV process, would be to:**\n",
    "1.  **Engineer richer features** (e.g., incorporate sales data, external economic indicators).\n",
    "2.  **Use a more appropriate time-series model** (e.g., an **RNN/LSTM** or a **Tree-based model** like XGBoost, which often handle time-series complexity better than simple NNs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
