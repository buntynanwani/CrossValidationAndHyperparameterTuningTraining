{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "887a889d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c40893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 500 samples for regression.\n",
      "Goal: Minimize Mean Absolute Error (MAE).\n"
     ]
    }
   ],
   "source": [
    "## üìö 1. Setup and Data Loading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV # NEW TOOL!\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "# --- Load a standard regression dataset (Make Regression for large, fast demo) ---\n",
    "X, y = make_regression(\n",
    "    n_samples=500,        # 500 samples\n",
    "    n_features=10,        # 10 features\n",
    "    n_informative=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert MAE to a scorer (since we maximize score, we use negative MAE)\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "print(f\"Dataset loaded: {len(X)} samples for regression.\")\n",
    "print(\"Goal: Minimize Mean Absolute Error (MAE).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877fbedf",
   "metadata": {},
   "source": [
    "## üé≤ 2. Introduction to Random Search\n",
    "\n",
    "**Random Search** is a powerful and efficient alternative to Grid Search. Instead of testing *every* single combination in a predefined grid:\n",
    "\n",
    "1.  It randomly samples a **fixed number of combinations** from the specified hyperparameter distributions.\n",
    "2.  It uses Cross-Validation to score only those randomly sampled combinations.\n",
    "\n",
    "### üß† Why Random Search is Better (Often)\n",
    "\n",
    "Grid Search wastes time testing redundant points (e.g., trying a shallow tree with many split points). Random Search focuses its limited computational budget on exploring different *regions* of the parameter space, often finding a near-optimal or even better solution faster than Grid Search.\n",
    "\n",
    "We will compare the time taken and the final score achieved using the same overall search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecbf7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothetical Full Grid Size: 288 models.\n"
     ]
    }
   ],
   "source": [
    "## üõ†Ô∏è 3. Defining the Search Space\n",
    "\n",
    "# We use a Random Forest Regressor since our main project is regression\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# We define the search space (which is large to make the comparison meaningful)\n",
    "# Note: n_estimators and max_depth are the key tuning parameters for RF.\n",
    "param_distribution = {\n",
    "    'n_estimators': [50, 100, 150, 200],                  # 4 values (trees in the forest)\n",
    "    'max_depth': [5, 10, 15, 20, 30, None],               # 6 values (max depth of each tree)\n",
    "    'min_samples_split': [2, 5, 10, 20],                  # 4 values\n",
    "    'max_features': [1.0, 'sqrt', 'log2']                 # 3 values\n",
    "}\n",
    "\n",
    "# The hypothetical full grid size is: 4 * 6 * 4 * 3 = 288 combinations\n",
    "print(f\"Hypothetical Full Grid Size: {4 * 6 * 4 * 3} models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25140a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Random Search (40 iterations * 5 folds = 200 trainings)...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    }
   ],
   "source": [
    "## üìä 4. Implementing RandomizedSearchCV\n",
    "\n",
    "# Set up the Cross-Validation strategy (Standard KFold for general regression)\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- Random Search Specifics ---\n",
    "n_iterations = 40  # We will test only 40 random combinations (out of 288 possible)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,                 # Random Forest Regressor\n",
    "    param_distributions=param_distribution, # The search space distribution\n",
    "    n_iter=n_iterations,                # The fixed number of random combinations to test\n",
    "    scoring=mae_scorer,                 # Metric: Negative MAE (to be maximized)\n",
    "    cv=cv_strategy,                     # CV strategy\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting Random Search ({n_iterations} iterations * 5 folds = {n_iterations * 5} trainings)...\")\n",
    "start_time_random = time.time()\n",
    "random_search.fit(X, y)\n",
    "end_time_random = time.time()\n",
    "random_time = end_time_random - start_time_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b06227d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Search Results ---\n",
      "Time Taken: 63.16 seconds.\n",
      "Best CV Score (MAE): 18.892\n",
      "Best Hyperparameters Found: {'n_estimators': 200, 'min_samples_split': 2, 'max_features': 1.0, 'max_depth': 20}\n",
      "\n",
      "--- Efficiency Comparison ---\n",
      "Random Search Tested: 40 combinations.\n",
      "Hypothetical Grid Search Time (288 combos): 454.72 seconds.\n",
      "\n",
      "Random Search was 7.2x faster while achieving a competitive score!\n"
     ]
    }
   ],
   "source": [
    "## üìà 5. Analyzing and Comparing Results\n",
    "\n",
    "# 5.1. Random Search Results\n",
    "print(\"\\n--- Random Search Results ---\")\n",
    "print(f\"Time Taken: {random_time:.2f} seconds.\")\n",
    "# Since scoring is negative MAE, we make it positive for readability\n",
    "print(f\"Best CV Score (MAE): {-random_search.best_score_:.3f}\") \n",
    "print(f\"Best Hyperparameters Found: {random_search.best_params_}\")\n",
    "\n",
    "# 5.2. Time Comparison (Hypothetical Grid Search)\n",
    "# Hypothetical time: (Total Combinations / Tested Combinations) * Random Search Time\n",
    "total_combos = 288\n",
    "hypothetical_grid_time = (total_combos / n_iterations) * random_time\n",
    "\n",
    "print(f\"\\n--- Efficiency Comparison ---\")\n",
    "print(f\"Random Search Tested: {n_iterations} combinations.\")\n",
    "print(f\"Hypothetical Grid Search Time ({total_combos} combos): {hypothetical_grid_time:.2f} seconds.\")\n",
    "\n",
    "# Final Comparison\n",
    "print(f\"\\nRandom Search was {hypothetical_grid_time / random_time:.1f}x faster while achieving a competitive score!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf9566",
   "metadata": {},
   "source": [
    "## üåü 6. Conclusion and Dataset Rationale\n",
    "\n",
    "### The Power of Random Search\n",
    "\n",
    "* **Efficiency:** Random Search is dramatically faster than Grid Search, especially as the number of hyperparameters and their possible values increases. By focusing its budget, it often finds a near-optimal solution without having to test the entire space.\n",
    "* **Recommendation:** For most real-world tuning tasks, **Random Search is the first technique you should try.**\n",
    "\n",
    "### üéØ Rationale for Dataset Choice\n",
    "\n",
    "| Dataset | Problem Type | Why we used it in this notebook | Why we did NOT use your data |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **`make_regression`** | **General Regression** | It's a clean, fast-to-generate dataset with 500 samples. This allows us to run many cross-validation folds quickly to accurately measure the **time efficiency** of Random Search vs. Grid Search. | It's **Time Series** data. Using it here would require the slow `TimeSeriesSplit` CV, skewing the time comparison and confusing the core lesson (Random vs. Grid). |\n",
    "| **`Supplement_Sales...`** | **Time Series Regression** | N/A | Tuning requires **`TimeSeriesSplit`** CV, which we will use in a dedicated, slower tuning notebook when the focus shifts from \"efficiency comparison\" to \"final model deployment.\" |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
