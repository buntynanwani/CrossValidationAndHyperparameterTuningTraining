{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfcbe76a",
   "metadata": {},
   "source": [
    "# üéì Introduction to Cross-Validation: The Problem It Solves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e45b6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data loaded successfully. Shape: (4384, 10)\n",
      "Features engineered. Ready for modeling. Final shape: (51, 14)\n"
     ]
    }
   ],
   "source": [
    "## üìö 1. Setup and Data Loading (with Feature Engineering)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# --- Data Loading ---\n",
    "file_path = '../../datasets/Supplement_Sales_Weekly_Expanded.csv'\n",
    "try:\n",
    "    data = pd.read_csv(file_path) \n",
    "    print(\"Raw data loaded successfully. Shape:\", data.shape)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Please check the path to your prepared dataset.\")\n",
    "    # Exit or raise error if loading fails to prevent subsequent KeyErrors\n",
    "\n",
    "# --- Feature Engineering (Based on your Steps 1 & 2) ---\n",
    "\n",
    "# 1. Data Cleaning and Preparation (Temporal & Grouping)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data = data.drop(columns=['Category', 'Revenue', 'Location'], errors='ignore')\n",
    "\n",
    "# Calculate the average monthly price (as per your notes)\n",
    "product_data_grouped = data.groupby(['Product_Name', 'Year', 'Month']).agg(\n",
    "    Price_Avg=('Price', 'mean'), # Calculate the average price\n",
    "    Product_ID=('Product_Name', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# Sort data chronologically for lag/time features\n",
    "product_data_grouped = product_data_grouped.sort_values(by=['Product_Name', 'Year', 'Month']).reset_index(drop=True)\n",
    "\n",
    "# Select a single product for simplicity in this demo\n",
    "PRODUCT_ID = product_data_grouped['Product_Name'].unique()[0]\n",
    "product_data = product_data_grouped[product_data_grouped['Product_Name'] == PRODUCT_ID].copy()\n",
    "\n",
    "\n",
    "# 2. Creating Additional Features (Time & Lag)\n",
    "product_data['Time_Index'] = np.arange(len(product_data)) + 1\n",
    "product_data['Time_Index_Squared'] = product_data['Time_Index'] ** 2\n",
    "\n",
    "# Seasonal Coding\n",
    "product_data['Month_sin'] = np.sin(2 * np.pi * product_data['Month'] / 12)\n",
    "product_data['Month_cos'] = np.cos(2 * np.pi * product_data['Month'] / 12)\n",
    "\n",
    "# Lag Variables (using .shift())\n",
    "product_data['Price_Lag_1'] = product_data['Price_Avg'].shift(1)\n",
    "product_data['Price_Lag_3'] = product_data['Price_Avg'].shift(3)\n",
    "product_data['Price_Lag_12'] = product_data['Price_Avg'].shift(12)\n",
    "\n",
    "# Moving Averages (using .rolling())\n",
    "product_data['Price_MA_6'] = product_data['Price_Avg'].rolling(window=6).mean().shift(1)\n",
    "product_data['Price_MA_12'] = product_data['Price_Avg'].rolling(window=12).mean().shift(1)\n",
    "\n",
    "# Drop initial rows with NaN values created by lags/MA\n",
    "product_data = product_data.dropna().reset_index(drop=True)\n",
    "print(\"Features engineered. Ready for modeling. Final shape:\", product_data.shape)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "FEATURES = ['Year', 'Month', 'Month_sin', 'Month_cos', 'Time_Index', 'Time_Index_Squared', \n",
    "            'Price_Lag_1', 'Price_Lag_3', 'Price_Lag_12', 'Price_MA_6', 'Price_MA_12']\n",
    "TARGET = 'Price_Avg'\n",
    "\n",
    "X = product_data[FEATURES]\n",
    "y = product_data[TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dfe410",
   "metadata": {},
   "source": [
    "# üéì 2. The Problem Cross-Validation Solves (Overfitting)\n",
    "\n",
    "We are predicting the average price of a vitamin supplement using a Random Forest model.\n",
    "\n",
    "### üß† The Exam Analogy and Overfitting\n",
    "\n",
    "Imagine your **Model is a student** and your **Data is the exam material**.\n",
    "\n",
    "> If you test your model using the **same data** it used to learn (study), the model will get a great score. This is called **overfitting**‚Äîthe model has simply **memorized the answers** instead of learning the general concepts.\n",
    ">\n",
    "> That great score is a **fake measure** of how well the model will perform on **new, unseen data** in the real world.\n",
    "\n",
    "### ‚ö†Ô∏è The Problem with a Single Split\n",
    "\n",
    "In our original project, we used a single **80/20 train-test split**, respecting the chronological order (training on older data, testing on newer data).\n",
    "\n",
    "While respecting time is good, relying on **just one single split** has a major flaw:\n",
    "\n",
    "1.  **Luck/Unluck:** If the 20% test period happens to contain unusual, scattered prices (noise), our final score will look worse than it really is.\n",
    "2.  **No Confidence Interval:** We get one score (e.g., MAE = 0.50). We don't know if that score is stable or just a lucky/unlucky result from that specific time window.\n",
    "\n",
    "**Cross-Validation fixes this by performing many fair, independent tests.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00eb92c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 40 samples. Test size: 11 samples.\n",
      "\n",
      "Single Train-Test MAE (Mean Absolute Error): $4.998\n"
     ]
    }
   ],
   "source": [
    "## üìâ 3. The Unreliable Single Train-Test Split\n",
    "\n",
    "# 3.1. Single split (Chronological)\n",
    "# Since this is time-series data, we split chronologically (80% for train, 20% for test)\n",
    "split_point = int(len(X) * 0.80)\n",
    "X_train, X_test = X[:split_point], X[split_point:]\n",
    "y_train, y_test = y[:split_point], y[split_point:]\n",
    "\n",
    "print(f\"Train size: {len(X_train)} samples. Test size: {len(X_test)} samples.\")\n",
    "\n",
    "\n",
    "# 3.2. Train and Evaluate\n",
    "model_single = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_single.fit(X_train, y_train)\n",
    "\n",
    "y_pred_single = model_single.predict(X_test)\n",
    "\n",
    "mae_single = mean_absolute_error(y_test, y_pred_single)\n",
    "print(f\"\\nSingle Train-Test MAE (Mean Absolute Error): ${mae_single:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992efbfb",
   "metadata": {},
   "source": [
    "# 3.3. Interpretation (Markdown Cell)\n",
    "### Interpretation of Single Split\n",
    "\n",
    "We achieved a Mean Absolute Error (MAE) of **[Insert MAE from above]**.\n",
    "\n",
    "**Question:** Is this a robust score? What if we had tested on a different 20% period? We have no way to know.\n",
    "\n",
    "This is why we need Cross-Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad3d068",
   "metadata": {},
   "source": [
    "## üîÅ 4. Introducing K-Fold Cross-Validation\n",
    "\n",
    "Instead of one single test, K-Fold CV gives our model **multiple, independent tests**.\n",
    "\n",
    "We split the entire dataset into **K** equal pieces (folds). We then run **K** separate experiments, rotating which piece is used for testing:\n",
    "\n",
    "| Experiment | Training Data | Testing Data (Validation) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Fold 1** | Folds 2, 3, 4, 5 | **Fold 1** |\n",
    "| **Fold 2** | Folds 1, 3, 4, 5 | **Fold 2** |\n",
    "| ... | ... | ... |\n",
    "\n",
    "**Note on Time Series:** For true time-series data, standard K-Fold is usually not correct because it mixes past and future data. However, for a basic introduction, we will use it here to demonstrate the *averaging* concept. (We will address the proper time-series CV in a later notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f8be921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual MAE scores for each fold (test):\n",
      "[7.61504818 4.311373   8.2899025  5.60471    4.758919  ]\n",
      "\n",
      "Final CV Score (Average MAE): $6.116\n",
      "Standard Deviation of MAE: 1.571\n"
     ]
    }
   ],
   "source": [
    "## üìä 5. Implementing K-Fold CV (5 Folds)\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Use K=5 for this introduction\n",
    "kf = KFold(n_splits=5, shuffle=False) # shuffle=False maintains the time order within folds, though the folds themselves are not strictly chronological splits.\n",
    "\n",
    "# Re-initialize the model\n",
    "model_cv = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Use cross_val_score: Note that 'neg_mean_absolute_error' is used because scikit-learn\n",
    "# treats scoring metrics as something to be maximized (higher is better).\n",
    "# The negative sign converts the error into a \"score.\"\n",
    "cv_scores = cross_val_score(\n",
    "    model_cv, \n",
    "    X, \n",
    "    y, \n",
    "    cv=kf, \n",
    "    scoring='neg_mean_absolute_error'\n",
    ")\n",
    "\n",
    "# Convert negative scores back to positive MAE errors\n",
    "cv_maes = -cv_scores \n",
    "\n",
    "print(\"Individual MAE scores for each fold (test):\")\n",
    "print(cv_maes)\n",
    "\n",
    "print(f\"\\nFinal CV Score (Average MAE): ${cv_maes.mean():.3f}\")\n",
    "print(f\"Standard Deviation of MAE: {cv_maes.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c64cbb7",
   "metadata": {},
   "source": [
    "## **üåü 6\\. Conclusion: A Reliable Score**\n",
    "\n",
    "Now we can directly compare the result from the single, potentially unreliable test against the more robust Cross-Validation (CV) average.\n",
    "\n",
    "| Metric | Single Split (80/20) Result | K-Fold CV (Average) Result |\n",
    "| :---- | :---- | :---- |\n",
    "| **MAE** | **$4.998** | **$6.116** |\n",
    "| **Robustness** | Low (Based on one single test period) | High (Based on 5 different test periods) |\n",
    "\n",
    "### **üß† What These Results Tell Us**\n",
    "\n",
    "1. **The Single Split was Optimistic (and Likely Unreliable):**  \n",
    "   * Your initial single test score of **$4.998** was quite low. This suggests that the final 20% of the data used for the test might have been an **easier, less noisy period** for the model to predict.  \n",
    "   * If you had relied only on that $4.998 score, you would have **overestimated** your model's real-world accuracy.  \n",
    "2. **The CV Score is the Honest Grade:**  \n",
    "   * The **Average MAE of $6.116** is the model's true, general performance. This score is much more trustworthy because it ensures that **every part of your data** has been used fairly for testing.  \n",
    "3. **The Model's Performance Varies:**  \n",
    "   * The **Individual MAE Scores** ranged from **$4.311** to **$8.290**.  \n",
    "   * The **Standard Deviation of 1.571** shows that the model's prediction accuracy changes significantly depending on the time period it's tested on. A high standard deviation means the model is **not perfectly stable**.  \n",
    "   * This is a strong sign that the model may be struggling with high **variance** (a form of instability), which is exactly what CV is designed to expose\\!\n",
    "\n",
    "**In summary, Cross-Validation gave your model an honest, overall grade of $6.116, revealing that its performance is less stable than the single initial test suggested.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80879ce",
   "metadata": {},
   "source": [
    "# üéì Introduction to Cross-Validation: The Problem It Solves\n",
    "\n",
    "## üîç Concept\n",
    "\n",
    "**Can We Trust a Single Test Score?** Cross-Validation solves the fundamental problem of unreliable model evaluation by testing on multiple data splits instead of just one.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Key Points\n",
    "\n",
    "### The Problem: Overfitting & Unreliable Evaluation\n",
    "\n",
    "**The Student Analogy**:\n",
    "- üìö Model = Student studying for an exam\n",
    "- üìù Training Data = Study materials\n",
    "- ‚úÖ Test Data = The actual exam\n",
    "\n",
    "**What Goes Wrong with Single Split**:\n",
    "- Student memorizes answers instead of learning concepts (overfitting)\n",
    "- Testing on same data = letting student grade their own homework\n",
    "- One test period might be unusually easy or hard (luck/bad luck)\n",
    "- No confidence interval - just one number\n",
    "\n",
    "### Dataset & Setup\n",
    "- **Data**: `Supplement_Sales_Weekly_Expanded.csv`\n",
    "- **Samples**: 51 time-series observations (after feature engineering from 4,384 rows)\n",
    "- **Model**: RandomForestRegressor (n_estimators=100)\n",
    "- **Target**: Predict average monthly supplement price\n",
    "- **Features**: Time indices, lags (1, 3, 12 months), moving averages (6, 12 months), seasonality (sin/cos)\n",
    "\n",
    "### The Experiment: Single Split vs K-Fold CV\n",
    "\n",
    "**Single Train-Test Split (80/20 chronological)**:\n",
    "```\n",
    "Training: 40 samples ‚Üí Testing: 11 samples\n",
    "Result: MAE = $4.998\n",
    "Question: Is this reliable?\n",
    "```\n",
    "\n",
    "**K-Fold Cross-Validation (K=5)**:\n",
    "```\n",
    "5 independent tests, each using different data\n",
    "Every sample gets tested exactly once\n",
    "Result: Average MAE = $6.116 ¬± $1.571\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Results Comparison\n",
    "\n",
    "### Single Split vs Cross-Validation\n",
    "\n",
    "| Metric | Single Split (80/20) | K-Fold CV (5 folds) | Difference |\n",
    "|--------|---------------------|---------------------|------------|\n",
    "| **MAE** | **$4.998** | **$6.116** | +22% error |\n",
    "| **Std Dev** | Unknown (only 1 test) | **¬±$1.571** | High variance |\n",
    "| **Confidence** | ‚ùå Low (lucky period?) | ‚úÖ High (5 tests) |\n",
    "| **Robustness** | ‚ùå Based on 1 period | ‚úÖ Based on 5 periods |\n",
    "\n",
    "### Individual Fold Performance (K-Fold CV)\n",
    "\n",
    "```\n",
    "Fold 1: $7.62  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ (worst)\n",
    "Fold 2: $4.31  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ (best)\n",
    "Fold 3: $8.29  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ (catastrophic)\n",
    "Fold 4: $5.60  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Fold 5: $4.76  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "Average: $6.116\n",
    "Range: $3.98 (from best to worst)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Findings & Insights\n",
    "\n",
    "### 1. The Single Split Was Deceptively Good\n",
    "‚úÖ **Single Split**: MAE = $4.998 looked great!  \n",
    "‚ùå **Reality**: That 20% test period was **unusually easy** to predict  \n",
    "‚ö†Ô∏è **Risk**: Would have overestimated model quality by 22%\n",
    "\n",
    "**What Happened**: The final 11 samples (20% test set) happened to have:\n",
    "- Lower price volatility\n",
    "- Smoother trends\n",
    "- Fewer market anomalies\n",
    "\n",
    "### 2. Cross-Validation Revealed the Truth\n",
    "‚úÖ **Average MAE**: $6.116 (realistic performance)  \n",
    "‚úÖ **Std Dev**: ¬±$1.571 (model is unstable!)  \n",
    "‚úÖ **Range**: $4.31 to $8.29 (huge variability)\n",
    "\n",
    "**What This Means**:\n",
    "- Model performs **inconsistently** across time periods\n",
    "- Some periods are easy ($4.31), others catastrophic ($8.29)\n",
    "- **High variance** in predictions = unreliable for production\n",
    "\n",
    "### 3. The Model Has Stability Issues\n",
    "**Standard Deviation of ¬±$1.571** is concerning:\n",
    "- Represents 26% variability relative to average error\n",
    "- Model's accuracy **depends heavily** on which time period it predicts\n",
    "- Suggests the model hasn't learned robust, generalizable patterns\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Visualization: How CV Works\n",
    "\n",
    "### Single Split (Unreliable)\n",
    "```\n",
    "Timeline: [‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê]\n",
    "          |‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ80%‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ|‚îÄ‚îÄ20%‚îÄ‚îÄ|\n",
    "          Training (40 samples)        Test (11 samples)\n",
    "          \n",
    "Result: MAE = $4.998 ‚Üê Based on ONE lucky test period\n",
    "```\n",
    "\n",
    "### K-Fold Cross-Validation (Reliable)\n",
    "```\n",
    "Fold 1: [‚ïê‚ïê‚ïê‚ïê‚ïê‚ïêTest‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê][‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïêTrain‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê]\n",
    "Fold 2: [‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïêTrain‚ïê‚ïê‚ïê‚ïê][‚ïê‚ïêTest‚ïê‚ïê][‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïêTrain‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê]\n",
    "Fold 3: [‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïêTrain‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê][‚ïê‚ïêTest‚ïê‚ïê][‚ïê‚ïê‚ïêTrain‚ïê‚ïê‚ïê‚ïê]\n",
    "Fold 4: [‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïêTrain‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê][‚ïê‚ïêTest‚ïê‚ïê][Train]\n",
    "Fold 5: [‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïêTrain‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê][‚ïê‚ïêTest]\n",
    "\n",
    "Result: Average MAE = $6.116 ‚Üê Based on 5 diverse test periods\n",
    "        Every sample tested exactly once\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì The Learning Experience\n",
    "\n",
    "### The \"Exam Analogy\" Explained\n",
    "\n",
    "**‚ùå Bad Testing (Single Split)**:\n",
    "- Like giving a student ONE practice test\n",
    "- They might get lucky with easy questions\n",
    "- You think they're brilliant, but they just got lucky\n",
    "\n",
    "**‚úÖ Good Testing (Cross-Validation)**:\n",
    "- Like giving 5 different exams\n",
    "- Student must perform well on ALL of them\n",
    "- Average score is their TRUE ability level\n",
    "\n",
    "### Why the Single Split Failed\n",
    "\n",
    "**The 20% test set was like an easy exam**:\n",
    "- Contained a stable price period (low volatility)\n",
    "- Few market shocks or anomalies\n",
    "- Model appeared better than it really was\n",
    "\n",
    "**Cross-Validation tested ALL conditions**:\n",
    "- Stable periods (Fold 2: $4.31)\n",
    "- Volatile periods (Fold 3: $8.29)\n",
    "- Average conditions (Fold 4/5: ~$5)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Critical Insights\n",
    "\n",
    "### What We Discovered\n",
    "\n",
    "**1. Luck Matters in Single Splits**\n",
    "> If we had only trusted the $4.998 single-split result, we would have deployed a model that's actually 22% worse in real-world conditions.\n",
    "\n",
    "**2. Variability Reveals Instability**\n",
    "> The ¬±$1.571 standard deviation shows the model is **brittle** - it works well in some market conditions but fails in others (like Fold 3's $8.29).\n",
    "\n",
    "**3. Cross-Validation is Essential**\n",
    "> For time-series data with limited samples (51 observations), testing on multiple periods is the ONLY way to know if your model generalizes.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Implications & Next Steps\n",
    "\n",
    "### For This Model\n",
    "\n",
    "**Current Status**:\n",
    "- ‚úÖ Cross-Validation implemented correctly\n",
    "- ‚ö†Ô∏è Model shows high variance (unstable)\n",
    "- ‚ùå Not production-ready without improvements\n",
    "\n",
    "**Why High Variance Occurred**:\n",
    "- **Limited Data**: Only 51 samples after feature engineering\n",
    "- **Market Complexity**: Supplement prices driven by external factors\n",
    "- **Model Architecture**: Random Forest may be overfit to training patterns\n",
    "\n",
    "### Recommended Actions\n",
    "\n",
    "**Short-term**:\n",
    "1. ‚úÖ **Use CV Score ($6.12)** not single split ($5.00) for reporting\n",
    "2. ‚è≠Ô∏è Try simpler models (Linear Regression, Ridge) for comparison\n",
    "3. ‚è≠Ô∏è Add more features (external market data, seasonality indicators)\n",
    "4. ‚è≠Ô∏è Collect more data if possible (increase from 51 samples)\n",
    "\n",
    "**Long-term**:\n",
    "1. Implement **TimeSeriesSplit** (proper temporal CV - Notebook 03)\n",
    "2. Try ensemble methods to reduce variance\n",
    "3. Test neural networks if more data available\n",
    "4. Add confidence intervals to predictions (¬±$1.57 error band)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What This Notebook Teaches\n",
    "\n",
    "### Core Concepts Demonstrated\n",
    "\n",
    "1. **Overfitting Recognition**: Single split can hide overfitting\n",
    "2. **Evaluation Reliability**: Multiple tests > one test\n",
    "3. **Performance Variance**: Models perform differently across periods\n",
    "4. **Honest Metrics**: Average of many tests = true performance\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "**Before Cross-Validation**:\n",
    "- \"Our model has $5 error - great!\" ‚Üê False confidence\n",
    "- Deploy to production\n",
    "- Realize errors are actually $6-8 in real use\n",
    "- Business loses trust in ML\n",
    "\n",
    "**After Cross-Validation**:\n",
    "- \"Our model has $6.12 error with ¬±$1.57 variability\" ‚Üê Honest\n",
    "- Improve model before deployment\n",
    "- Set realistic expectations ($5-8 error range)\n",
    "- Business gets reliable predictions\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "### The Main Lesson\n",
    "> **Never trust a single train-test split.** Always use Cross-Validation to get multiple, independent performance estimates. The average score is your model's TRUE capability.\n",
    "\n",
    "### Specific Numbers to Remember\n",
    "- **Single Split**: $4.998 (22% too optimistic)\n",
    "- **CV Average**: $6.116 (honest performance)\n",
    "- **CV Std Dev**: ¬±$1.571 (model is unstable)\n",
    "- **Range**: $4.31 to $8.29 (huge variability)\n",
    "\n",
    "### Why This Foundation Matters\n",
    "\n",
    "This notebook sets up the entire project:\n",
    "- **Notebook 01** (this one): Why CV matters\n",
    "- **Notebook 02**: Stratified K-Fold for classification\n",
    "- **Notebook 03**: TimeSeriesSplit for proper temporal validation ‚≠ê\n",
    "- **Notebook 04-05**: Hyperparameter tuning WITH CV\n",
    "\n",
    "Without understanding this problem, you can't appreciate why Notebook 03's TimeSeriesSplit ($5.08 MAE) is the REAL winner.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Final Comparison: The Truth Revealed\n",
    "\n",
    "| Evaluation Method | MAE | Reliability | Decision |\n",
    "|------------------|-----|-------------|----------|\n",
    "| **Single Split** | $4.998 | ‚ùå Unreliable (lucky) | Would deploy bad model |\n",
    "| **K-Fold CV** | $6.116 ¬± $1.571 | ‚ö†Ô∏è Reliable but shows instability | Know model needs work |\n",
    "| **TimeSeriesSplit** (Notebook 03) | **$5.08 ¬± $0.92** | ‚úÖ Most reliable & stable | **Best for deployment** |\n",
    "\n",
    "**Conclusion**: Cross-Validation saved us from a 22% overestimation of model quality!\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Educational Value\n",
    "\n",
    "**This notebook is foundational because it:**\n",
    "1. ‚úÖ Shows the **danger** of single splits (false confidence)\n",
    "2. ‚úÖ Demonstrates **how CV works** (multiple independent tests)\n",
    "3. ‚úÖ Reveals **model instability** (high variance across folds)\n",
    "4. ‚úÖ Sets up the need for **proper time-series CV** (Notebook 03)\n",
    "5. ‚úÖ Teaches **honest evaluation** (report averages & std dev)\n",
    "\n",
    "**Connection to Final Results**:\n",
    "- This K-Fold CV ($6.12) was still not ideal (ignores temporal order)\n",
    "- TimeSeriesSplit (Notebook 03) gave $5.08 - even better!\n",
    "- Neural Network (Notebook 05) gave $8.38 - worse than all CV methods\n",
    "- **Moral**: Proper CV reveals truth, single splits hide it\n",
    "\n",
    "---\n",
    "\n",
    "*This is why Cross-Validation is the foundation of reliable machine learning!* üéØ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
