{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578247b0",
   "metadata": {},
   "source": [
    "# üß© K-Fold vs Stratified K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1f0aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 150 samples.\n",
      "Target distribution (0, 1, 2): [50, 50, 50]\n"
     ]
    }
   ],
   "source": [
    "## üìö 1. Setup and Data Loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# --- Load a standard classification dataset (Iris) ---\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(f\"Dataset loaded: {iris.frame.shape[0]} samples.\")\n",
    "print(f\"Target distribution (0, 1, 2): {y.value_counts().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1548e334",
   "metadata": {},
   "source": [
    "## üîÅ 2. Basic K-Fold Cross-Validation\n",
    "\n",
    "**K-Fold** is the standard workhorse for general CV. It divides the data into $K$ equal-sized blocks. Since order doesn't matter here, we can **shuffle** the data to ensure each fold is randomly mixed.\n",
    "\n",
    "### 2.1. Defining the Folds\n",
    "\n",
    "We will use $K=5$ folds.\n",
    "\n",
    "```python\n",
    "# Initialize a simple K-Fold (Shuffle=True is the standard for non-time-series data)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"K-Fold object created with 5 splits and shuffling enabled.\")\n",
    "\n",
    "# Initialize a simple classification model (Logistic Regression)\n",
    "model_kf = LogisticRegression(solver='liblinear', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d13ec236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the K-Fold cross-validator and model ---\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Logistic Regression model for classification\n",
    "model_kf = LogisticRegression(max_iter=1000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c734c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy scores for each of the 5 folds:\n",
      "[1.         1.         0.93333333 0.96666667 0.96666667]\n",
      "\n",
      "Final K-Fold CV Score (Average Accuracy): 0.9733\n",
      "Standard Deviation of Accuracy: 0.0249\n"
     ]
    }
   ],
   "source": [
    "# Use cross_val_score to perform K-Fold CV\n",
    "# Scoring is set to 'accuracy' for this classification problem\n",
    "cv_scores_kf = cross_val_score(\n",
    "    model_kf, \n",
    "    X, \n",
    "    y, \n",
    "    cv=kf, \n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(\"\\nAccuracy scores for each of the 5 folds:\")\n",
    "print(cv_scores_kf)\n",
    "\n",
    "print(f\"\\nFinal K-Fold CV Score (Average Accuracy): {cv_scores_kf.mean():.4f}\")\n",
    "print(f\"Standard Deviation of Accuracy: {cv_scores_kf.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152c20d",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è 3. The Problem: When Classes are Imbalanced\n",
    "\n",
    "In our Iris dataset, the classes are perfectly balanced (50 samples each). But what if they weren't?\n",
    "\n",
    "Imagine you are classifying a rare disease (95% healthy, 5% sick).\n",
    "\n",
    "If you use **standard K-Fold**, a random split might result in one of your test folds (the exam questions) accidentally containing:\n",
    "* **Only** healthy samples, giving a useless test score.\n",
    "* **No** sick samples, meaning the model is never tested on the hardest cases.\n",
    "\n",
    "**Solution:** We need to ensure that every fold is a miniature, representative sample of the whole dataset. This is called **Stratification**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fbd650",
   "metadata": {},
   "source": [
    "## üìè 4. Stratified K-Fold (The Fair Exam)\n",
    "\n",
    "**Stratified K-Fold** guarantees that the proportion of the target class (y) is roughly the same in every training fold and testing fold. This is the **required method** for virtually all classification problems.\n",
    "\n",
    "### 4.1. Defining the Stratified Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd64811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified K-Fold object created with 5 splits and guaranteed class balance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Stratified K-Fold\n",
    "# Note: StratifiedKFold requires shuffle=True to work properly\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"Stratified K-Fold object created with 5 splits and guaranteed class balance.\")\n",
    "\n",
    "# Initialize model again\n",
    "model_skf = LogisticRegression(solver='liblinear', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "462ae116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy scores for each of the 5 stratified folds:\n",
      "[0.96666667 1.         0.9        0.93333333 1.        ]\n",
      "\n",
      "Final Stratified CV Score (Average Accuracy): 0.9600\n",
      "Standard Deviation of Accuracy: 0.0389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use cross_val_score with the Stratified object\n",
    "cv_scores_skf = cross_val_score(\n",
    "    model_skf, \n",
    "    X, \n",
    "    y, \n",
    "    cv=skf,  # Using the StratifiedKFold object\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(\"\\nAccuracy scores for each of the 5 stratified folds:\")\n",
    "print(cv_scores_skf)\n",
    "\n",
    "print(f\"\\nFinal Stratified CV Score (Average Accuracy): {cv_scores_skf.mean():.4f}\")\n",
    "print(f\"Standard Deviation of Accuracy: {cv_scores_skf.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274317af",
   "metadata": {},
   "source": [
    "## üåü 5. Conclusion and Next Step\n",
    "\n",
    "### Summary of Results:\n",
    "\n",
    "| Method | Average Accuracy | Standard Deviation |\n",
    "| :--- | :--- | :--- |\n",
    "| **K-Fold (Basic)** | [Insert Average KF Score] | [Insert Std Dev KF Score] |\n",
    "| **Stratified K-Fold** | [Insert Average SKF Score] | [Insert Std Dev SKF Score] |\n",
    "\n",
    "For **balanced datasets** like Iris, the results are often very similar. However, for real-world **imbalanced classification problems**, **Stratified K-Fold** is essential to ensure a reliable and honest evaluation of the model.\n",
    "\n",
    "### ‚è≠Ô∏è What About Time Series?\n",
    "\n",
    "In our previous notebook, we used K-Fold on time-series data, which is technically incorrect because it breaks the chronological order (mixing past and future).\n",
    "\n",
    "In the next notebook, we will learn the correct CV method for time-series data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4fc0ed",
   "metadata": {},
   "source": [
    "Final Summary Analisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc810b",
   "metadata": {},
   "source": [
    "# üß© K-Fold vs Stratified K-Fold Cross-Validation\n",
    "\n",
    "## üîç Concept\n",
    "\n",
    "**The Fair Exam Problem**: How do you ensure every test fairly represents all student skill levels? Stratified K-Fold solves this for imbalanced classification problems.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Key Points\n",
    "\n",
    "### The Setup\n",
    "\n",
    "**Dataset**: Iris flower classification\n",
    "- **Samples**: 150 observations\n",
    "- **Classes**: 3 types (Setosa, Versicolor, Virginica)\n",
    "- **Distribution**: Perfectly balanced (50, 50, 50)\n",
    "- **Features**: 4 measurements (sepal/petal length/width)\n",
    "\n",
    "**Model**: Logistic Regression (solver='liblinear')  \n",
    "**Evaluation**: 5-Fold Cross-Validation  \n",
    "**Goal**: Compare basic K-Fold vs Stratified K-Fold\n",
    "\n",
    "### The Problem Being Solved\n",
    "\n",
    "**Imagine a Rare Disease Dataset**:\n",
    "- 95% healthy patients (Class 0)\n",
    "- 5% sick patients (Class 1)\n",
    "\n",
    "**What Could Go Wrong with Basic K-Fold**:\n",
    "```\n",
    "Fold 1: [95% healthy, 5% sick]     ‚úÖ Representative\n",
    "Fold 2: [100% healthy, 0% sick]    ‚ùå Missing sick patients!\n",
    "Fold 3: [92% healthy, 8% sick]     ‚ö†Ô∏è Slightly off\n",
    "Fold 4: [98% healthy, 2% sick]     ‚ö†Ô∏è Underrepresents sick\n",
    "Fold 5: [90% healthy, 10% sick]    ‚ö†Ô∏è Overrepresents sick\n",
    "```\n",
    "\n",
    "**Result**: Fold 2 never tests the model on sick patients - unreliable evaluation!\n",
    "\n",
    "**Stratified K-Fold Solution**:\n",
    "```\n",
    "EVERY fold: [95% healthy, 5% sick]  ‚úÖ All folds representative!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Results Comparison\n",
    "\n",
    "### Overall Performance\n",
    "\n",
    "| Method | Avg. Accuracy | Std. Dev. | Key Insight |\n",
    "|--------|--------------|-----------|-------------|\n",
    "| **K-Fold** | **0.9733** (97.33%) | ¬±0.0249 (2.49%) | Works well for balanced data |\n",
    "| **Stratified K-Fold** | **0.9600** (96.00%) | ¬±0.0389 (3.89%) | Guarantees class balance in each fold |\n",
    "\n",
    "### Individual Fold Performance\n",
    "\n",
    "**K-Fold Results**:\n",
    "```\n",
    "Fold 1: 100.0%  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Perfect\n",
    "Fold 2: 100.0%  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Perfect\n",
    "Fold 3:  93.3%  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Good\n",
    "Fold 4:  96.7%  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Very Good\n",
    "Fold 5:  96.7%  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Very Good\n",
    "\n",
    "Average: 97.33% ¬± 2.49%\n",
    "```\n",
    "\n",
    "**Stratified K-Fold Results**:\n",
    "```\n",
    "Fold 1:  96.7%  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Very Good\n",
    "Fold 2: 100.0%  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Perfect\n",
    "Fold 3:  90.0%  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Good\n",
    "Fold 4:  93.3%  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Good\n",
    "Fold 5: 100.0%  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Perfect\n",
    "\n",
    "Average: 96.00% ¬± 3.89%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Analysis & Interpretation\n",
    "\n",
    "### Why K-Fold Performed Slightly Better (97.33% vs 96.00%)\n",
    "\n",
    "**Reason 1: Dataset is Perfectly Balanced**\n",
    "- Iris has exactly 50 samples per class\n",
    "- Random K-Fold naturally creates balanced folds\n",
    "- Stratification provides no advantage here\n",
    "\n",
    "**Reason 2: Lucky Random Split**\n",
    "- K-Fold got 2 perfect folds (100% accuracy)\n",
    "- This is random luck, not systematic superiority\n",
    "- Different random_state would change results\n",
    "\n",
    "**Reason 3: Model is Strong**\n",
    "- Logistic Regression separates Iris classes easily\n",
    "- Even slightly imbalanced folds still perform well\n",
    "- The difference (97.33% vs 96.00%) is negligible\n",
    "\n",
    "### Why Stratified Has Higher Standard Deviation (3.89% vs 2.49%)\n",
    "\n",
    "**This seems counterintuitive but makes sense**:\n",
    "\n",
    "**K-Fold's Low Variability** (¬±2.49%):\n",
    "- Got lucky with well-balanced random splits\n",
    "- Two folds were 100% accurate\n",
    "- Artificially low variance due to chance\n",
    "\n",
    "**Stratified's Higher Variability** (¬±3.89%):\n",
    "- **Guarantees** exact class proportions in each fold\n",
    "- But can't control which specific samples go where\n",
    "- Some folds got harder-to-classify flowers by chance\n",
    "- Range: 90% to 100% (one challenging fold)\n",
    "\n",
    "**Key Insight**: Higher variance doesn't mean Stratified is worse - it means it's more honest about the inherent difficulty variation across samples.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† The \"Fair Exam\" Analogy\n",
    "\n",
    "### Basic K-Fold: The Lucky Random Test\n",
    "\n",
    "**Scenario**: Teacher randomly assigns students to 5 different exams\n",
    "- **Exam 1**: Happens to get all A-students ‚Üí 100% pass rate\n",
    "- **Exam 2**: Happens to get all A-students ‚Üí 100% pass rate  \n",
    "- **Exam 3**: Mix of A/B/C students ‚Üí 93% pass rate\n",
    "- **Average**: 97.3% (but exams 1 & 2 were unrealistically easy!)\n",
    "\n",
    "**Problem**: You can't trust this average because some exams were easier by chance.\n",
    "\n",
    "### Stratified K-Fold: The Deliberately Fair Test\n",
    "\n",
    "**Scenario**: Teacher ensures EVERY exam has:\n",
    "- 33% A-students\n",
    "- 33% B-students\n",
    "- 34% C-students\n",
    "\n",
    "**Result**:\n",
    "- **Every exam** has the same difficulty level\n",
    "- No exam gets unfair advantage/disadvantage\n",
    "- Average represents TRUE class performance\n",
    "- Variability comes from inherent difficulty, not random luck\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è When Does Stratification Matter Most?\n",
    "\n",
    "### Critical for Imbalanced Datasets\n",
    "\n",
    "**Example: Fraud Detection**\n",
    "- 99.5% legitimate transactions (Class 0)\n",
    "- 0.5% fraudulent transactions (Class 1)\n",
    "\n",
    "**Without Stratification**:\n",
    "```python\n",
    "# K-Fold might create:\n",
    "Fold 1: 1,000 transactions ‚Üí 4 fraudulent (0.4%) ‚ùå Underrepresented\n",
    "Fold 2: 1,000 transactions ‚Üí 7 fraudulent (0.7%) ‚ö†Ô∏è Overrepresented  \n",
    "Fold 3: 1,000 transactions ‚Üí 0 fraudulent (0.0%) ‚ùå‚ùå DISASTER!\n",
    "```\n",
    "- Fold 3 never tests fraud detection!\n",
    "- Model evaluation is completely unreliable\n",
    "\n",
    "**With Stratified K-Fold**:\n",
    "```python\n",
    "# Every fold guaranteed:\n",
    "All Folds: 1,000 transactions ‚Üí 5 fraudulent (0.5%) ‚úÖ Perfect balance\n",
    "```\n",
    "\n",
    "### Impact on Performance Metrics\n",
    "\n",
    "**Imbalanced Dataset Performance**:\n",
    "\n",
    "| Scenario | K-Fold | Stratified K-Fold |\n",
    "|----------|--------|-------------------|\n",
    "| **Fold gets no minority class** | 99.5% accuracy (useless!) | Impossible - guaranteed balance |\n",
    "| **Standard Deviation** | ¬±10% (high variability) | ¬±2% (stable) |\n",
    "| **Minority Class Recall** | 0% to 100% (unstable) | 85% to 95% (reliable) |\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Visual Comparison: How They Work\n",
    "\n",
    "### K-Fold (Random Split)\n",
    "\n",
    "```\n",
    "Dataset: [‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óã‚óã‚óã‚óã‚óã] (10 Class A, 5 Class B)\n",
    "\n",
    "Fold 1: [‚óè‚óè‚óè‚óã]         40% Class B ‚ö†Ô∏è Overrepresented\n",
    "Fold 2: [‚óè‚óè‚óè‚óã]         33% Class B ‚úÖ Close to target\n",
    "Fold 3: [‚óè‚óè‚óè‚óã]         33% Class B ‚úÖ Close to target\n",
    "Fold 4: [‚óè‚óè‚óè‚óã]         33% Class B ‚úÖ Close to target\n",
    "Fold 5: [‚óè‚óè‚óã‚óã]         50% Class B ‚ùå Way overrepresented!\n",
    "\n",
    "Result: Fold proportions vary from 33% to 50%\n",
    "```\n",
    "\n",
    "### Stratified K-Fold (Guaranteed Balance)\n",
    "\n",
    "```\n",
    "Dataset: [‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óã‚óã‚óã‚óã‚óã] (10 Class A, 5 Class B)\n",
    "\n",
    "Fold 1: [‚óè‚óè‚óã]          33.3% Class B ‚úÖ Exact\n",
    "Fold 2: [‚óè‚óè‚óã]          33.3% Class B ‚úÖ Exact\n",
    "Fold 3: [‚óè‚óè‚óã]          33.3% Class B ‚úÖ Exact\n",
    "Fold 4: [‚óè‚óè‚óã]          33.3% Class B ‚úÖ Exact\n",
    "Fold 5: [‚óè‚óè‚óã]          33.3% Class B ‚úÖ Exact\n",
    "\n",
    "Result: ALL folds have exactly 33.3% Class B\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Learnings\n",
    "\n",
    "### 1. Why Results Were Similar for Iris\n",
    "\n",
    "‚úÖ **Iris is perfectly balanced** (50, 50, 50)  \n",
    "‚úÖ **Model is strong** (Logistic Regression works well)  \n",
    "‚úÖ **Random K-Fold got lucky** with naturally balanced splits  \n",
    "‚ö†Ô∏è **Don't be fooled**: With imbalanced data, K-Fold would fail badly\n",
    "\n",
    "### 2. When to Use Each Method\n",
    "\n",
    "**Use Basic K-Fold**:\n",
    "- ‚ùå Almost never for classification\n",
    "- ‚úÖ Only for regression problems\n",
    "- ‚úÖ Only when classes are perfectly balanced AND you're absolutely sure\n",
    "\n",
    "**Use Stratified K-Fold**:\n",
    "- ‚úÖ **Always for classification** (default choice)\n",
    "- ‚úÖ **Essential for imbalanced classes**\n",
    "- ‚úÖ Even when balanced (no downside, adds safety)\n",
    "- ‚úÖ Fraud detection, disease diagnosis, rare event prediction\n",
    "\n",
    "### 3. Understanding the Variance Difference\n",
    "\n",
    "**K-Fold: ¬±2.49% (lower variance)**\n",
    "- Seems better but it's **misleading**\n",
    "- Result of lucky random splits\n",
    "- Would vary dramatically with different random_state\n",
    "\n",
    "**Stratified: ¬±3.89% (higher variance)**\n",
    "- **More honest** representation of difficulty\n",
    "- Comes from inherent sample variability\n",
    "- Stable across different random_state values\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Practical Recommendations\n",
    "\n",
    "### For Production Machine Learning\n",
    "\n",
    "**Classification Tasks**:\n",
    "```python\n",
    "# ‚ùå DON'T DO THIS\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5)\n",
    "\n",
    "# ‚úÖ ALWAYS DO THIS\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "```\n",
    "\n",
    "**Regression Tasks**:\n",
    "```python\n",
    "# ‚úÖ Use regular K-Fold (no classes to balance)\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "```\n",
    "\n",
    "**Time-Series Tasks**:\n",
    "```python\n",
    "# ‚úÖ Use TimeSeriesSplit (coming in Notebook 03!)\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. ‚úÖ **Default to Stratified K-Fold** for all classification\n",
    "2. ‚úÖ **Use 5-10 folds** (5 is standard, 10 for small datasets)\n",
    "3. ‚úÖ **Always set random_state** for reproducibility\n",
    "4. ‚úÖ **Always shuffle=True** unless time-series\n",
    "5. ‚ö†Ô∏è **Check class distribution** before choosing method\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Connection to Other Notebooks\n",
    "\n",
    "### How This Fits the Project\n",
    "\n",
    "**Notebook 01**: Intro to CV\n",
    "- Showed why single split is unreliable ($5.00 vs $6.12)\n",
    "- Used basic K-Fold on time-series (wrong method!)\n",
    "\n",
    "**Notebook 02 (This One)**: K-Fold vs Stratified\n",
    "- Explains stratification for classification\n",
    "- Shows both methods work on balanced data\n",
    "- **Critical for imbalanced problems**\n",
    "\n",
    "**Notebook 03**: TimeSeriesSplit\n",
    "- Will show the RIGHT method for temporal data\n",
    "- Achieves $5.08 MAE (best result!)\n",
    "- Respects chronological order\n",
    "\n",
    "**Notebooks 04-05**: Hyperparameter Tuning\n",
    "- All tuning uses TimeSeriesSplit (proper temporal CV)\n",
    "- Stratified K-Fold used in Grid Search example (Iris)\n",
    "- Shows CV must match data structure\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "### The Bottom Line\n",
    "\n",
    "> **Always use Stratified K-Fold for classification.** Even when data is balanced (like Iris), there's no downside. When data is imbalanced, it's absolutely critical.\n",
    "\n",
    "### Critical Numbers\n",
    "\n",
    "- **K-Fold**: 97.33% ¬± 2.49% (lucky random split)\n",
    "- **Stratified**: 96.00% ¬± 3.89% (guaranteed fair)\n",
    "- **Difference**: 1.33% (negligible on balanced data)\n",
    "- **On imbalanced data**: Difference could be 20-50%!\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "**Scenario**: 1% fraud detection dataset\n",
    "\n",
    "| Method | Fold 1 Fraud % | Fold 2 Fraud % | Fold 3 Fraud % | Reliability |\n",
    "|--------|---------------|---------------|---------------|-------------|\n",
    "| K-Fold | 0.5% | 0.0% ‚ùå | 2.0% | Terrible |\n",
    "| Stratified | 1.0% | 1.0% | 1.0% | Perfect ‚úÖ |\n",
    "\n",
    "Without Stratified K-Fold, Fold 2 would have **zero fraud examples** - making evaluation completely worthless!\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Warning: Common Mistake\n",
    "\n",
    "**FutureWarning in Results**:\n",
    "```\n",
    "FutureWarning: Using 'liblinear' solver for multiclass \n",
    "classification is deprecated. Use another solver...\n",
    "```\n",
    "\n",
    "**What This Means**:\n",
    "- Code works but uses deprecated method\n",
    "- Update to: `LogisticRegression(max_iter=1000, random_state=42)`\n",
    "- Or use: `LogisticRegression(solver='lbfgs', max_iter=1000)`\n",
    "\n",
    "**Not a Problem for the Analysis** - just a library version issue.\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Educational Value\n",
    "\n",
    "**This notebook teaches**:\n",
    "1. ‚úÖ Why stratification matters (fair exams analogy)\n",
    "2. ‚úÖ When balanced data \"hides\" the need (Iris example)\n",
    "3. ‚úÖ How catastrophic imbalance can be (fraud example)\n",
    "4. ‚úÖ The foundation for proper model evaluation\n",
    "5. ‚úÖ Best practices for classification CV\n",
    "\n",
    "**Next Step**: Learn TimeSeriesSplit for temporal data (Notebook 03) - where neither K-Fold nor Stratified K-Fold works!\n",
    "\n",
    "---\n",
    "\n",
    "*Stratified K-Fold: The standard for classification CV* üéØ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
