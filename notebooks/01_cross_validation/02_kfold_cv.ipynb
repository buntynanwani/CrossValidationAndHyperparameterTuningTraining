{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578247b0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1f0aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 150 samples.\n",
      "Target distribution (0, 1, 2): [50, 50, 50]\n"
     ]
    }
   ],
   "source": [
    "## üìö 1. Setup and Data Loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# --- Load a standard classification dataset (Iris) ---\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(f\"Dataset loaded: {iris.frame.shape[0]} samples.\")\n",
    "print(f\"Target distribution (0, 1, 2): {y.value_counts().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1548e334",
   "metadata": {},
   "source": [
    "## üîÅ 2. Basic K-Fold Cross-Validation\n",
    "\n",
    "**K-Fold** is the standard workhorse for general CV. It divides the data into $K$ equal-sized blocks. Since order doesn't matter here, we can **shuffle** the data to ensure each fold is randomly mixed.\n",
    "\n",
    "### 2.1. Defining the Folds\n",
    "\n",
    "We will use $K=5$ folds.\n",
    "\n",
    "```python\n",
    "# Initialize a simple K-Fold (Shuffle=True is the standard for non-time-series data)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"K-Fold object created with 5 splits and shuffling enabled.\")\n",
    "\n",
    "# Initialize a simple classification model (Logistic Regression)\n",
    "model_kf = LogisticRegression(solver='liblinear', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d13ec236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the K-Fold cross-validator and model ---\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Logistic Regression model for classification\n",
    "model_kf = LogisticRegression(max_iter=1000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c734c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy scores for each of the 5 folds:\n",
      "[1.         1.         0.93333333 0.96666667 0.96666667]\n",
      "\n",
      "Final K-Fold CV Score (Average Accuracy): 0.9733\n",
      "Standard Deviation of Accuracy: 0.0249\n"
     ]
    }
   ],
   "source": [
    "# Use cross_val_score to perform K-Fold CV\n",
    "# Scoring is set to 'accuracy' for this classification problem\n",
    "cv_scores_kf = cross_val_score(\n",
    "    model_kf, \n",
    "    X, \n",
    "    y, \n",
    "    cv=kf, \n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(\"\\nAccuracy scores for each of the 5 folds:\")\n",
    "print(cv_scores_kf)\n",
    "\n",
    "print(f\"\\nFinal K-Fold CV Score (Average Accuracy): {cv_scores_kf.mean():.4f}\")\n",
    "print(f\"Standard Deviation of Accuracy: {cv_scores_kf.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152c20d",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è 3. The Problem: When Classes are Imbalanced\n",
    "\n",
    "In our Iris dataset, the classes are perfectly balanced (50 samples each). But what if they weren't?\n",
    "\n",
    "Imagine you are classifying a rare disease (95% healthy, 5% sick).\n",
    "\n",
    "If you use **standard K-Fold**, a random split might result in one of your test folds (the exam questions) accidentally containing:\n",
    "* **Only** healthy samples, giving a useless test score.\n",
    "* **No** sick samples, meaning the model is never tested on the hardest cases.\n",
    "\n",
    "**Solution:** We need to ensure that every fold is a miniature, representative sample of the whole dataset. This is called **Stratification**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fbd650",
   "metadata": {},
   "source": [
    "## üìè 4. Stratified K-Fold (The Fair Exam)\n",
    "\n",
    "**Stratified K-Fold** guarantees that the proportion of the target class (y) is roughly the same in every training fold and testing fold. This is the **required method** for virtually all classification problems.\n",
    "\n",
    "### 4.1. Defining the Stratified Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd64811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified K-Fold object created with 5 splits and guaranteed class balance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Stratified K-Fold\n",
    "# Note: StratifiedKFold requires shuffle=True to work properly\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"Stratified K-Fold object created with 5 splits and guaranteed class balance.\")\n",
    "\n",
    "# Initialize model again\n",
    "model_skf = LogisticRegression(solver='liblinear', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "462ae116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy scores for each of the 5 stratified folds:\n",
      "[0.96666667 1.         0.9        0.93333333 1.        ]\n",
      "\n",
      "Final Stratified CV Score (Average Accuracy): 0.9600\n",
      "Standard Deviation of Accuracy: 0.0389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\f5\\CrossValidationAndHyperparameterTuningTraining\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use cross_val_score with the Stratified object\n",
    "cv_scores_skf = cross_val_score(\n",
    "    model_skf, \n",
    "    X, \n",
    "    y, \n",
    "    cv=skf,  # Using the StratifiedKFold object\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(\"\\nAccuracy scores for each of the 5 stratified folds:\")\n",
    "print(cv_scores_skf)\n",
    "\n",
    "print(f\"\\nFinal Stratified CV Score (Average Accuracy): {cv_scores_skf.mean():.4f}\")\n",
    "print(f\"Standard Deviation of Accuracy: {cv_scores_skf.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274317af",
   "metadata": {},
   "source": [
    "## üåü 5. Conclusion and Next Step\n",
    "\n",
    "### Summary of Results:\n",
    "\n",
    "| Method | Average Accuracy | Standard Deviation |\n",
    "| :--- | :--- | :--- |\n",
    "| **K-Fold (Basic)** | [Insert Average KF Score] | [Insert Std Dev KF Score] |\n",
    "| **Stratified K-Fold** | [Insert Average SKF Score] | [Insert Std Dev SKF Score] |\n",
    "\n",
    "For **balanced datasets** like Iris, the results are often very similar. However, for real-world **imbalanced classification problems**, **Stratified K-Fold** is essential to ensure a reliable and honest evaluation of the model.\n",
    "\n",
    "### ‚è≠Ô∏è What About Time Series?\n",
    "\n",
    "In our previous notebook, we used K-Fold on time-series data, which is technically incorrect because it breaks the chronological order (mixing past and future).\n",
    "\n",
    "In the next notebook, we will learn the correct CV method for time-series data!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
