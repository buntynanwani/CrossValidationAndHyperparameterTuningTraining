# ğŸ“Š Cross-Validation & Hyperparameter Tuning: A Complete Learning Journey

**Dataset**: `Supplement_Sales_Weekly_Expanded.csv` (Time-Series Price Prediction)  
**Objective**: Master CV and tuning techniques through practical application  
**Date**: November 2025

---

## ğŸ¯ Executive Summary

This report documents a comprehensive exploration of Cross-Validation (CV) and Hyperparameter Tuning techniques applied to supplement price prediction. **The journey reveals both successes and instructive failures**, demonstrating why choosing the correct CV method for your data type is critical.

### The Complete Story Arc

âœ… **Started with**: Basic concepts (why CV matters)  
âš ï¸ **Encountered**: Common mistakes (wrong CV for time-series)  
ğŸ¯ **Discovered**: The correct approach (TimeSeriesSplit)  
âŒ **Learned from**: Over-engineering (Neural Networks failed)  
ğŸ† **Concluded**: Simple + Correct > Complex + Wrong

---

## ğŸ“‹ The Five Notebooks: A Learning Journey

### ğŸ“ Notebook 01: Introduction to Cross-Validation
**Lesson**: "Never Trust a Single Test Score"

**What We Did**:
- Compared single train-test split (80/20) vs K-Fold CV (5 folds)
- Used `Supplement_Sales_Weekly_Expanded.csv` time-series data
- Applied Random Forest Regressor

**Results**:
| Method | MAE | Std Dev | Reliability |
|--------|-----|---------|-------------|
| Single Split | **$5.00** | N/A | âŒ Unreliable (lucky period) |
| K-Fold CV | **$6.12** | Â±$1.57 | âš ï¸ WRONG METHOD! |

**Critical Discovery** âš ï¸:
- Single split looked great ($5.00) but tested only ONE lucky period
- K-Fold revealed instability (Â±$1.57 variance)
- **BUT K-Fold was the WRONG method for time-series data!**

**The Mistake**:
```python
# âŒ WRONG: K-Fold shuffles and mixes past/future
kf = KFold(n_splits=5, shuffle=False)
# This breaks temporal order â†’ data leakage!
```

**Key Learning**:
> "We proved CV is better than single split, BUT we used the wrong CV method. K-Fold mixes past and future data, causing data leakage!"

---

### ğŸ§© Notebook 02: Stratified K-Fold for Classification
**Lesson**: "Fair Exams for Imbalanced Classes"

**What We Did**:
- Switched to **Iris dataset** (classification, 150 samples)
- Compared K-Fold vs Stratified K-Fold
- Used Logistic Regression

**Results**:
| Method | Accuracy | Std Dev | Purpose |
|--------|----------|---------|---------|
| K-Fold | 97.33% | Â±2.49% | Random splits |
| Stratified K-Fold | 96.00% | Â±3.89% | Guaranteed class balance |

**Why Different Dataset**?
- **Supplement data** = Regression problem (predict prices)
- **Iris data** = Classification problem (predict flower types)
- **Purpose**: Show how Stratified K-Fold prevents class imbalance issues

**Critical Discovery** âœ…:
- For balanced data (Iris), both methods work similarly
- For imbalanced data (fraud detection), Stratified K-Fold is ESSENTIAL
- Example: Without stratification, one fold could have 0% fraud samples!

**Key Learning**:
> "Always use Stratified K-Fold for classification, but this doesn't apply to our time-series regression problem. We need a different solution!"

---

### â° Notebook 03: TimeSeriesSplit - THE WINNER
**Lesson**: "Never Train on the Future to Predict the Past"

**What We Did**:
- Returned to `Supplement_Sales_Weekly_Expanded.csv`
- Applied **TimeSeriesSplit** (expanding window)
- Used Random Forest Regressor
- Finally used the CORRECT CV method!

**Results**:
| Method | MAE | Std Dev | Validity |
|--------|-----|---------|----------|
| Single Split | $5.00 | N/A | âŒ Risky |
| K-Fold | $6.12 | Â±$1.57 | âŒ WRONG (leakage) |
| **TimeSeriesSplit** | **$5.08** | **Â±$0.92** | âœ… **CORRECT** â­ |

**Individual Fold Performance**:
```
Fold 1: $4.05  (11 training samples)
Fold 2: $6.60  (19 training samples - hit volatile period)
Fold 3: $4.53  (27 training samples)
Fold 4: $5.63  (35 training samples)
Fold 5: $4.62  (43 training samples)

Average: $5.08 Â± $0.92
```

**Critical Discovery** ğŸ†:
1. **Lowest Valid Error**: $5.08 (only honest temporal evaluation)
2. **Most Stable**: Â±$0.92 variance (41% better than K-Fold!)
3. **No Data Leakage**: Always trains on past, tests on future
4. **Production Ready**: Mimics real-world forecasting scenario

**Why K-Fold Failed**:
```
âŒ K-Fold Mixed Timeline:
Fold 1: Train on [Feb, Mar, Jun] â†’ Test on [Jan, Jul]
        (Training on June to predict January = time travel!)

âœ… TimeSeriesSplit Respects Order:
Fold 1: Train on [Jan-Mar] â†’ Test on [Apr-Jun]
Fold 2: Train on [Jan-Jun] â†’ Test on [Jul-Sep]
        (Always past â†’ future)
```

**Key Learning**:
> "TimeSeriesSplit achieved the BEST result ($5.08 Â± $0.92) by respecting temporal order. This is the ONLY valid method for time-series forecasting."

---

### ğŸ¤– Notebook 04: Optuna Neural Network Tuning
**Lesson**: "Advanced Tuning Doesn't Guarantee Success"

**What We Did**:
- Applied Optuna hyperparameter tuning (50 trials, ~50 minutes)
- Built Neural Network with TimeSeriesSplit (3 folds)
- Searched: layers (1-3), units (16-128), learning rate, dropout, batch size

**Optuna Tuning Results**:
| Metric | Value | Interpretation |
|--------|-------|----------------|
| Best Trial | #36 | Found after 36 attempts |
| Best Architecture | 1 layer, 39 units | Shallow network won! |
| Best MAE (scaled) | 0.6977 | Looked promising |
| Learning Rate | 0.0062 | Middle of search range |
| Time | 2,987 seconds | 50 minutes total |

**Critical Discovery** âš ï¸:
- **Optuna found optimal hyperparameters** for feed-forward NN
- **But only used 3 folds** (faster tuning, less rigorous)
- Result looked good (0.6977 scaled MAE)
- **Was this the real performance?** â†’ Notebook 05 reveals truth!

**Architecture Evolution**:
```
Early trials (0-20):  Complex (2-3 layers, 100+ units) â†’ MAE ~0.77
Mid trials (21-35):   Moderate (1-2 layers, 50-80 units) â†’ MAE ~0.71
Trial 36 (BEST):      Simple (1 layer, 39 units) â†’ MAE 0.6977 âœ…
```

**Key Learning**:
> "For small datasets (57 samples), Optuna consistently preferred SIMPLE architectures. Complexity doesn't equal performance - sometimes less is more!"

---

### ğŸ”¬ Notebook 05: Combined CV & Tuning - THE REALITY CHECK
**Lesson**: "Rigorous Validation Reveals Truth"

**What We Did**:
- Used best hyperparameters from Optuna (1 layer, 39 units, lr=0.0062)
- Applied **5-fold TimeSeriesSplit** (more rigorous than tuning's 3 folds)
- Tested Neural Network on actual supplement price prediction

**Final Evaluation Results**:
| Fold | Actual MAE | RÂ² Score | What Happened |
|------|-----------|----------|---------------|
| 1 | $7.76 | -1.32 | Poor |
| 2 | **$3.47** | 0.09 | âœ… Only decent fold |
| 3 | **$19.60** | **-11.77** | âŒ CATASTROPHIC |
| 4 | $5.57 | -0.29 | Below average |
| 5 | $5.50 | -0.03 | Mediocre |
| **Average** | **$8.38** | **-2.66** | âŒ **FAILED** |

**Critical Discovery** âŒ:
1. **Negative RÂ² (-2.66)**: Model worse than predicting average price!
2. **High Error ($8.38)**: 67% worse than TimeSeriesSplit simple model ($5.08)
3. **Extreme Variability**: $3.47 to $19.60 (model is brittle)
4. **Optuna Was Optimistic**: 0.6977 (3 folds) â†’ 1.2989 (5 folds)

**Why Neural Network Failed**:
```
âœ… Hyperparameters: OPTIMAL (Optuna found best for this architecture)
âŒ Architecture: WRONG (Feed-forward NN not ideal for time-series)
âŒ Data Size: TOO SMALL (57 samples insufficient for deep learning)
âŒ Features: INCOMPLETE (missing market factors driving prices)
âŒ Approach: OVER-ENGINEERED (should have tried simpler models first)
```

**Key Learning**:
> "Hyperparameter tuning â‰  good model. Optuna found the best settings for a fundamentally flawed approach. Negative RÂ² saved us from deploying a disaster!"

---

## ğŸ“Š Complete Results Comparison

### All Methods on Supplement Sales Data

| Method | Dataset | MAE | Std Dev | RÂ² | Validity | Status |
|--------|---------|-----|---------|-----|----------|--------|
| **Single Split** | Supplement | $5.00 | N/A | N/A | âš ï¸ Risky | Lucky period |
| **K-Fold** | Supplement | $6.12 | Â±$1.57 | N/A | âŒ WRONG | Data leakage |
| **Stratified** | *Iris* | 96% acc | Â±3.89% | N/A | âœ… Correct | Different problem |
| **TimeSeriesSplit** | Supplement | **$5.08** | **Â±$0.92** | N/A | âœ… **WINNER** â­ | Proper temporal CV |
| **Optuna Tuning** | Supplement | 0.6977 (scaled) | N/A | ~0.3 | âš ï¸ Optimistic | Only 3 folds |
| **Neural Network** | Supplement | **$8.38** | Huge | **-2.66** | âœ… Honest | Failed architecture |

---

## ğŸ“ The Complete Learning Arc

### Act 1: Discovering the Problem (Notebook 01)
**Question**: "Can we trust a single test score?"  
**Discovery**: No! Single split ($5.00) was 22% too optimistic  
**Mistake**: Used K-Fold on time-series (wrong method)  
**Learning**: CV is essential, but method matters

### Act 2: Understanding Classification (Notebook 02)
**Question**: "How do we handle imbalanced classes?"  
**Discovery**: Stratified K-Fold guarantees class proportions  
**Context Switch**: Switched to Iris (classification example)  
**Learning**: Different data types need different CV methods

### Act 3: Finding the Right Method (Notebook 03)
**Question**: "What's the correct CV for time-series?"  
**Discovery**: TimeSeriesSplit! ($5.08 Â± $0.92)  
**Victory**: Lowest error + most stable + no leakage  
**Learning**: Respecting temporal order is non-negotiable

### Act 4: Advanced Tuning (Notebook 04)
**Question**: "Can neural networks do better?"  
**Discovery**: Optuna found optimal hyperparameters (0.6977 scaled MAE)  
**Hope**: Complex models might capture patterns  
**Learning**: Tuning is powerful but computationally expensive

### Act 5: The Reality Check (Notebook 05)
**Question**: "Does the tuned neural network work?"  
**Discovery**: NO! ($8.38 MAE, RÂ² = -2.66)  
**Revelation**: Over-engineering failed, simplicity won  
**Learning**: Proper validation reveals truth before deployment

---

## ğŸ” Why Choosing the Correct CV Method Is CRITICAL

### The Three Fatal Mistakes and Their Consequences

#### âŒ Mistake 1: K-Fold on Time-Series (Notebook 01)

**What We Did**:
```python
kf = KFold(n_splits=5, shuffle=False)  # WRONG!
```

**The Problem**:
- K-Fold creates random/sequential splits
- Mixes past and future data
- Model trained on June data to predict January
- **Data leakage** gives false confidence

**The Consequence**:
- $6.12 MAE with Â±$1.57 variance
- Higher error despite "seeing the future"
- Unreliable, would fail in production

**Why It Happened**:
- Didn't understand time-series requirements
- Assumed K-Fold works for all regression problems
- Failed to recognize temporal dependencies

#### âš ï¸ Mistake 2: Single Split Optimism (Notebook 01)

**What We Did**:
```python
split_point = int(len(X) * 0.80)  # One split only
```

**The Problem**:
- Tested on only ONE time period (final 20%)
- That period happened to be stable/easy
- No way to know if lucky or truly good

**The Consequence**:
- $5.00 MAE looked great
- Would have overestimated quality by 22%
- False confidence for deployment

**Why It Happened**:
- Didn't have multiple tests to reveal variance
- Didn't account for temporal volatility
- Trusted luck instead of rigor

#### âŒ Mistake 3: Over-Engineering with Neural Networks (Notebook 05)

**What We Did**:
```python
# 50 trials, 50 minutes, complex architecture
model = build_neural_network(n_layers=1, n_units=39)
```

**The Problem**:
- Feed-forward NN wrong for time-series
- 57 samples too small for deep learning
- Missing key features (market factors)
- Assumed complexity = better performance

**The Consequence**:
- $8.38 MAE (67% WORSE than simple model!)
- RÂ² = -2.66 (worse than baseline)
- Catastrophic Fold 3 ($19.60 error)

**Why It Happened**:
- Didn't try simpler models first (ARIMA, XGBoost)
- Assumed neural networks would automatically work
- Ignored warning signs from small dataset

---

## âœ… The Correct Approach: What We Should Have Done

### The Optimal Workflow

```
Step 1: Understand Your Data
â”œâ”€ Is it time-ordered? â†’ YES (supplement sales over time)
â”œâ”€ Problem type? â†’ Regression (predict prices)
â””â”€ Sample size? â†’ 51 observations (SMALL!)

Step 2: Choose Correct CV Method
â”œâ”€ Time-series + Regression â†’ TimeSeriesSplit âœ…
â””â”€ NOT K-Fold (breaks temporal order) âŒ

Step 3: Start Simple
â”œâ”€ Try: Linear Regression â†’ MAE ?
â”œâ”€ Try: ARIMA/SARIMA â†’ MAE ?
â”œâ”€ Try: Random Forest â†’ MAE $5.08 âœ… WINNER!
â””â”€ Try: XGBoost â†’ MAE ?

Step 4: Only Then Try Complex Models
â”œâ”€ If simple models fail AND dataset is large enough
â”œâ”€ Try: LSTM/GRU (proper temporal memory)
â””â”€ Try: Neural Networks (requires >500 samples typically)

Step 5: Use Rigorous Validation
â”œâ”€ Use 5-10 folds (not just 3)
â”œâ”€ Test on multiple time periods
â””â”€ Accept negative RÂ² as truth (not failure)
```

---

## ğŸ“ˆ Visual Timeline of Our Journey

### Error Evolution Across Notebooks

```
Notebook 01: $6.12 (K-Fold - WRONG METHOD)
             â†“ "Wait, is this right for time-series?"
             
Notebook 02: 96% accuracy (Stratified K-Fold - DIFFERENT DATA)
             â†“ "This is for classification, not our problem"
             
Notebook 03: $5.08 (TimeSeriesSplit - CORRECT! â­)
             â†“ "This is the right method! Can we do better?"
             
Notebook 04: 0.6977 scaled MAE (Optuna - LOOKS PROMISING)
             â†“ "Neural network might beat simple model..."
             
Notebook 05: $8.38 (Neural Network - FAILED!)
             â†“ "Over-engineering backfired. Simple was best."

Final Conclusion: TimeSeriesSplit + Random Forest = WINNER ($5.08)
```

---

## ğŸ¯ Critical Lessons Learned

### Lesson 1: CV Method MUST Match Data Structure

| Data Type | Correct CV | Wrong CV | Consequence |
|-----------|-----------|----------|-------------|
| **Time-Series Regression** | TimeSeriesSplit | K-Fold | Data leakage, false confidence |
| **Classification (Balanced)** | K-Fold or Stratified | TimeSeriesSplit | Unnecessary complexity |
| **Classification (Imbalanced)** | Stratified K-Fold | Regular K-Fold | Missing minority classes |
| **Spatial Data** | GroupKFold | K-Fold | Spatial leakage |

**Our Mistake**: Used K-Fold on time-series â†’ $6.12 error with leakage  
**Our Fix**: Used TimeSeriesSplit â†’ $5.08 error, honest evaluation

### Lesson 2: Simple + Correct > Complex + Wrong

| Approach | Complexity | Correctness | Result |
|----------|-----------|-------------|--------|
| Random Forest + TimeSeriesSplit | Simple | âœ… Correct | $5.08 â­ WINNER |
| Neural Network + Optuna Tuning | Complex | âœ… Correct CV | $8.38 âŒ FAILED |
| Random Forest + K-Fold | Simple | âŒ Wrong CV | $6.12 âš ï¸ INVALID |

**Key Insight**: The SIMPLEST model with the CORRECT CV method won!

### Lesson 3: Rigorous Validation Prevents Production Disasters

**Without Proper CV** (Single Split $5.00):
```
Deploy model to production
â†’ Realize errors are actually $6-8 in real use
â†’ Business loses trust in ML
â†’ Customers get wrong price predictions
â†’ Revenue impact from bad forecasting
```

**With Proper CV** (TimeSeriesSplit $5.08 Â± $0.92):
```
Know realistic performance before deployment
â†’ Set correct expectations ($5 Â± $1 error band)
â†’ Deploy with confidence intervals
â†’ Business gets reliable predictions
â†’ Can improve model if needed before launch
```

**Without Final Validation** (Optuna 0.6977 looked good):
```
Trust tuning results
â†’ Deploy neural network
â†’ Discover RÂ² = -2.66 in production (worse than baseline!)
â†’ DISASTER
```

**With Final Validation** (Notebook 05 revealed truth):
```
Test with 5 folds instead of 3
â†’ Discover RÂ² = -2.66 BEFORE deployment
â†’ Saved from catastrophic failure
â†’ Pivot to proven TimeSeriesSplit method ($5.08)
```

---

## ğŸ† The Winner: TimeSeriesSplit + Random Forest

### Why This Combination Won

**1. Correct CV Method** âœ…
- TimeSeriesSplit respects temporal order
- No data leakage (always past â†’ future)
- Expanding window simulates production reality
- 5 folds for rigorous testing

**2. Appropriate Model Complexity** âœ…
- Random Forest: Powerful but not over-engineered
- Handles non-linearity without deep learning overhead
- Works well with small datasets (51 samples)
- Interpretable (can see feature importance)

**3. Stable Performance** âœ…
- Average MAE: $5.08 (realistic)
- Standard Deviation: Â±$0.92 (low variance)
- Range: $4.05 to $6.60 (only 1 outlier)
- Consistent across 4 of 5 folds

**4. Production Ready** âœ…
- No unrealistic expectations
- Can deploy with "$5 Â± $1 error" confidence
- Monitoring plan: Watch for Fold 2-like volatility
- Retrain strategy: Expanding window as new data arrives

---

## ğŸ“Š What Each Dataset Taught Us

### Supplement_Sales_Weekly_Expanded.csv (Main Dataset)

**Characteristics**:
- 51 time-series observations (after feature engineering)
- High price volatility ($4 to $20 swings)
- Temporal dependencies (lag features matter)
- External market factors not captured

**Used In**:
- âœ… Notebook 01: Intro to CV (learned single split unreliable)
- âŒ Notebook 02: K-Fold CV (WRONG - data leakage!)
- âœ… Notebook 03: TimeSeriesSplit (CORRECT - $5.08 winner!)
- âœ… Notebook 04: Optuna tuning (looked promising)
- âœ… Notebook 05: Final validation (revealed NN failure)

**What It Taught Us**:
1. Time-series needs special CV (TimeSeriesSplit)
2. Small datasets (<100 samples) challenging for deep learning
3. Simple models can outperform complex ones
4. Market volatility creates prediction challenges

### Iris Dataset (Classification Example)

**Characteristics**:
- 150 samples, 3 classes (perfectly balanced)
- 4 features (sepal/petal measurements)
- No temporal dependencies
- Classic classification problem

**Used In**:
- âœ… Notebook 02: Stratified K-Fold (taught class balancing)

**What It Taught Us**:
1. Classification needs different CV than regression
2. Stratified K-Fold essential for imbalanced classes
3. On balanced data, regular K-Fold works too
4. Different data types need different techniques

---

## ğŸš€ Recommendations for Future Work

### For This Supplement Dataset

**Short-term (1-2 months)**:
1. âœ… Deploy TimeSeriesSplit + Random Forest ($5.08 MAE)
2. Collect more data (target: 200+ samples)
3. Add external features:
   - Competitor prices
   - Market indices
   - Google Trends data
   - Promotional calendar
4. Try XGBoost/LightGBM (often beat RF on tabular data)

**Long-term (3-6 months)**:
1. Test SARIMA (designed for seasonal time-series)
2. Try LSTM/GRU if data reaches 200+ samples
3. Implement ensemble of multiple models
4. Add confidence intervals to predictions

### For Any ML Project

**The Golden Rules**:

1. **Know Your Data Structure**
   - Time-series? â†’ TimeSeriesSplit
   - Classification (imbalanced)? â†’ Stratified K-Fold
   - Regression (no time)? â†’ K-Fold
   - Spatial data? â†’ GroupKFold

2. **Start Simple, Then Complexify**
   - Linear models first (baseline)
   - Tree-based methods second (Random Forest, XGBoost)
   - Deep learning LAST (and only if warranted)

3. **Use Rigorous Validation**
   - 5-10 folds minimum (not 3)
   - Multiple time periods for time-series
   - Accept negative RÂ² as truth
   - Report mean AND standard deviation

4. **Question Your Results**
   - Too good? â†’ Check for data leakage
   - High variance? â†’ Model unstable
   - Negative RÂ²? â†’ Wrong architecture
   - Single test? â†’ Not enough evidence

---

## ğŸ’¡ The Most Important Insight

### Why This Project is a Success Despite "Failures"

**What Looks Like Failure**:
- âŒ K-Fold had data leakage ($6.12)
- âŒ Single split was too optimistic ($5.00)
- âŒ Neural network completely failed ($8.38, RÂ² = -2.66)

**What Is Actually Success**:
- âœ… **Discovered** data leakage before production
- âœ… **Identified** single split unreliability through CV
- âœ… **Prevented** neural network deployment disaster
- âœ… **Found** the correct method (TimeSeriesSplit $5.08)
- âœ… **Learned** when complex models are inappropriate

### The Real Victory

> **We didn't just find the best model ($5.08 MAE). We learned WHY it's best, WHEN to use different methods, and HOW to avoid common pitfalls. The "failures" taught us more than lucky successes ever could.**

---

## ğŸ“Š Final Summary Table

| Aspect | Journey | Outcome |
|--------|---------|---------|
| **Best Method** | TimeSeriesSplit ($5.08 Â± $0.92) | Deploy this! â­ |
| **Worst Mistake** | K-Fold on time-series ($6.12 leakage) | Never repeat! |
| **Biggest Surprise** | Neural Network failed ($8.38 > $5.08) | Simple won! |
| **Key Learning** | CV method must match data structure | CRITICAL! |
| **Time Investment** | ~55 minutes (mostly Optuna tuning) | Worth it! |
| **Production Ready** | Yes - TimeSeriesSplit validated | Deploy now âœ… |

---

## ğŸ“ Conclusion: The Complete Picture

### What We Achieved

1. âœ… **Mastered CV Fundamentals**: Single split â†’ K-Fold â†’ Stratified â†’ TimeSeriesSplit
2. âœ… **Learned from Mistakes**: Data leakage â†’ Correct method selection
3. âœ… **Explored Hyperparameter Tuning**: Grid Search â†’ Random Search â†’ Bayesian â†’ Optuna
4. âœ… **Discovered Simplicity Wins**: $5.08 (simple) beat $8.38 (complex)
5. âœ… **Prevented Production Disaster**: Rigorous validation caught failures early

### The Three Pillars of Success

**1. Correct CV Method** (TimeSeriesSplit)
- Respects temporal order
- No data leakage
- Production-realistic evaluation

**2. Appropriate Model Complexity** (Random Forest)
- Powerful enough to capture patterns
- Simple enough for small datasets
- Interpretable and reliable

**3. Rigorous Validation** (5 folds)
- Multiple time periods tested
- Honest performance metrics
- Catches failures before deployment

---

## ğŸ¯ The Final Word

**For the Supplement Sales Dataset**:
- **WINNER**: TimeSeriesSplit + Random Forest = $5.08 Â± $0.92 â­
- **DEPLOY**: With confidence interval ($5 Â± $1 error band)
- **MONITOR**: Watch for volatile periods (like Fold 2)
- **IMPROVE**: Add features, collect more data, try XGBoost

**For All Machine Learning Projects**:
> **Choose your CV method as carefully as you choose your model. Wrong CV = Wrong conclusions, even with perfect hyperparameters. Always match your validation strategy to your data structure, start with simple models, and trust rigorous evaluation over lucky results.**

---

**This is not a story of failure - it's a story of learning done right.** ğŸ“âœ…

*Report Complete: November 2025*
